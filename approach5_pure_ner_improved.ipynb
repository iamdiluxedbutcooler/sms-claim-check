{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2648204",
   "metadata": {},
   "source": [
    "# Approach 5: Pure NER Claim Extraction (No Context Filtering)\n",
    "\n",
    "## Overview\n",
    "- Extract claims directly using NER model\n",
    "- Fixed dataset: Removed 31 near-duplicates\n",
    "- Grouped rare claims into OTHER_CLAIM\n",
    "- Fixed word-splitting issues with better BIO alignment\n",
    "\n",
    "## Improvements:\n",
    "1. Resolved 112 overlapping spans\n",
    "2. Better tokenization alignment (fixes \"Hur\", \"ry\", \"!\" splitting)\n",
    "3. Uses balanced 2000 dataset (1000 HAM, 1000 SMISH)\n",
    "4. No duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc24d2b",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063439ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets accelerate seqeval scikit-learn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c079e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c55b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "save_dir = '/content/drive/MyDrive/sms_claim_models/approach5_pure_ner'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(f\"Models will be saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a569d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Please upload 'claim_annotations_2000_balanced.json'\")\n",
    "uploaded = files.upload()\n",
    "data_file = list(uploaded.keys())[0]\n",
    "print(f\"Uploaded: {data_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16987a",
   "metadata": {},
   "source": [
    "## 2. Define Claim Types and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962b3912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group rare claims to handle data scarcity\n",
    "CLAIM_TYPES = [\n",
    "    'ACTION_CLAIM',\n",
    "    'URGENCY_CLAIM',\n",
    "    'REWARD_CLAIM',\n",
    "    'FINANCIAL_CLAIM',\n",
    "    'ACCOUNT_CLAIM',\n",
    "    'DELIVERY_CLAIM',\n",
    "    'VERIFICATION_CLAIM',\n",
    "    'OTHER_CLAIM'  # Groups: SECURITY, IDENTITY, CREDENTIALS, LEGAL, SOCIAL\n",
    "]\n",
    "\n",
    "RARE_CLAIMS = ['SECURITY_CLAIM', 'IDENTITY_CLAIM', 'CREDENTIALS_CLAIM', 'LEGAL_CLAIM', 'SOCIAL_CLAIM']\n",
    "\n",
    "def normalize_claim_type(claim_type):\n",
    "    return 'OTHER_CLAIM' if claim_type in RARE_CLAIMS else claim_type\n",
    "\n",
    "# Create BIO labels\n",
    "labels = ['O']\n",
    "for claim_type in CLAIM_TYPES:\n",
    "    labels.append(f'B-{claim_type}')\n",
    "    labels.append(f'I-{claim_type}')\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"Total labels: {len(labels)}\")\n",
    "print(f\"Claim types: {CLAIM_TYPES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc84d64",
   "metadata": {},
   "source": [
    "## 3. Data Loading with Overlap Resolution\n",
    "\n",
    "KEY FIX: Better BIO tagging to prevent word splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a78ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_overlapping_spans(claim_spans):\n",
    "    \"\"\"Resolve overlapping claim spans by keeping longer ones\"\"\"\n",
    "    if not claim_spans:\n",
    "        return []\n",
    "    \n",
    "    sorted_spans = sorted(claim_spans, key=lambda x: (x['start'], -len(x['text'])))\n",
    "    resolved = []\n",
    "    \n",
    "    for span in sorted_spans:\n",
    "        has_overlap = False\n",
    "        for existing in resolved:\n",
    "            if not (span['end'] <= existing['start'] or span['start'] >= existing['end']):\n",
    "                if len(span['text']) > len(existing['text']):\n",
    "                    resolved.remove(existing)\n",
    "                    resolved.append(span)\n",
    "                has_overlap = True\n",
    "                break\n",
    "        \n",
    "        if not has_overlap:\n",
    "            resolved.append(span)\n",
    "    \n",
    "    return sorted(resolved, key=lambda x: x['start'])\n",
    "\n",
    "def convert_to_bio_format(text, claim_spans):\n",
    "    \"\"\"\n",
    "    Convert text and claim spans to BIO format\n",
    "    FIX: Use character-level labeling to prevent word splitting\n",
    "    \"\"\"\n",
    "    claim_spans = resolve_overlapping_spans(claim_spans)\n",
    "    \n",
    "    # Create character-level labels\n",
    "    char_labels = ['O'] * len(text)\n",
    "    \n",
    "    for span in claim_spans:\n",
    "        claim_label = normalize_claim_type(span['label'])\n",
    "        start = span['start']\n",
    "        end = span['end']\n",
    "        \n",
    "        # Mark first character as B-, rest as I-\n",
    "        if start < len(char_labels):\n",
    "            char_labels[start] = f'B-{claim_label}'\n",
    "        \n",
    "        for i in range(start + 1, min(end, len(char_labels))):\n",
    "            char_labels[i] = f'I-{claim_label}'\n",
    "    \n",
    "    # Convert to word-level for compatibility\n",
    "    words = text.split()\n",
    "    word_labels = []\n",
    "    char_pos = 0\n",
    "    \n",
    "    for word in words:\n",
    "        word_start = text.find(word, char_pos)\n",
    "        if word_start == -1:\n",
    "            word_labels.append('O')\n",
    "            continue\n",
    "        \n",
    "        # Use label of first character of word\n",
    "        word_labels.append(char_labels[word_start])\n",
    "        char_pos = word_start + len(word)\n",
    "    \n",
    "    return words, word_labels, char_labels\n",
    "\n",
    "print(\"BIO conversion functions loaded with improved alignment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba01ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_claim_data(json_file):\n",
    "    \"\"\"Load claim annotations with overlap resolution\"\"\"\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    examples = []\n",
    "    overlaps_resolved = 0\n",
    "    \n",
    "    for entry in data:\n",
    "        text = entry['data']['text']\n",
    "        \n",
    "        if not entry.get('annotations') or len(entry['annotations']) == 0:\n",
    "            continue\n",
    "        \n",
    "        annotations = entry['annotations'][0]\n",
    "        claim_spans = []\n",
    "        \n",
    "        if 'result' in annotations and annotations['result']:\n",
    "            for result in annotations['result']:\n",
    "                value = result.get('value', {})\n",
    "                labels_list = value.get('labels', [])\n",
    "                \n",
    "                if labels_list:\n",
    "                    claim_spans.append({\n",
    "                        'text': value.get('text', ''),\n",
    "                        'start': value.get('start', 0),\n",
    "                        'end': value.get('end', 0),\n",
    "                        'label': labels_list[0]\n",
    "                    })\n",
    "        \n",
    "        original_count = len(claim_spans)\n",
    "        claim_spans = resolve_overlapping_spans(claim_spans)\n",
    "        if len(claim_spans) < original_count:\n",
    "            overlaps_resolved += (original_count - len(claim_spans))\n",
    "        \n",
    "        tokens, bio_labels, char_labels = convert_to_bio_format(text, claim_spans)\n",
    "        \n",
    "        examples.append({\n",
    "            'id': entry.get('id'),\n",
    "            'text': text,\n",
    "            'tokens': tokens,\n",
    "            'labels': bio_labels,\n",
    "            'char_labels': char_labels,\n",
    "            'claim_spans': claim_spans\n",
    "        })\n",
    "    \n",
    "    print(f\"Resolved {overlaps_resolved} overlapping spans\")\n",
    "    return examples\n",
    "\n",
    "print(\"Loading data...\")\n",
    "examples = load_claim_data(data_file)\n",
    "print(f\"Loaded {len(examples)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97868187",
   "metadata": {},
   "source": [
    "## 4. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split\n",
    "example_labels = []\n",
    "for ex in examples:\n",
    "    is_smish = len(ex.get('claim_spans', [])) > 0\n",
    "    example_labels.append('SMISH' if is_smish else 'HAM')\n",
    "\n",
    "train_examples, test_examples = train_test_split(\n",
    "    examples,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=example_labels\n",
    ")\n",
    "\n",
    "print(f\"Dataset split:\")\n",
    "print(f\"  Train: {len(train_examples)}\")\n",
    "print(f\"  Test: {len(test_examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7ae445",
   "metadata": {},
   "source": [
    "## 5. Tokenization with Fixed Alignment\n",
    "\n",
    "KEY FIX: Better subword token alignment using character-level labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ac952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n",
    "print(f\"Loaded tokenizer: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31789f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenize and align labels using character-level mapping\n",
    "    FIX: Uses char_labels directly to prevent word splitting issues\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        [ex['text'] for ex in examples],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_offsets_mapping=True,\n",
    "        is_split_into_words=False\n",
    "    )\n",
    "    \n",
    "    aligned_labels = []\n",
    "    \n",
    "    for i, example in enumerate(examples):\n",
    "        offset_mapping = tokenized_inputs['offset_mapping'][i]\n",
    "        char_labels = example['char_labels']\n",
    "        \n",
    "        labels = []\n",
    "        for start, end in offset_mapping:\n",
    "            if start == 0 and end == 0:\n",
    "                # Special token\n",
    "                labels.append(-100)\n",
    "            else:\n",
    "                # Use label of first character in this token\n",
    "                if start < len(char_labels):\n",
    "                    label = char_labels[start]\n",
    "                    labels.append(label2id.get(label, 0))\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "        \n",
    "        aligned_labels.append(labels)\n",
    "    \n",
    "    tokenized_inputs.pop('offset_mapping')\n",
    "    tokenized_inputs['labels'] = aligned_labels\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_tokenized = tokenize_and_align_labels(train_examples)\n",
    "test_tokenized = tokenize_and_align_labels(test_examples)\n",
    "print(\"Tokenization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9a254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "train_dataset = NERDataset(train_tokenized)\n",
    "test_dataset = NERDataset(test_tokenized)\n",
    "\n",
    "print(f\"Created datasets: Train={len(train_dataset)}, Test={len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dedbf3",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"Loaded model: {MODEL_NAME}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d0e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        true_label = []\n",
    "        pred_label = []\n",
    "        \n",
    "        for p, l in zip(prediction, label):\n",
    "            if l != -100:\n",
    "                true_label.append(id2label[l])\n",
    "                pred_label.append(id2label[p])\n",
    "        \n",
    "        true_labels.append(true_label)\n",
    "        pred_labels.append(pred_label)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, pred_labels),\n",
    "        \"recall\": recall_score(true_labels, pred_labels),\n",
    "        \"f1\": f1_score(true_labels, pred_labels),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments - IMPROVED for better accuracy\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./claim-ner-model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    learning_rate=2e-5,  # Slightly lower for better convergence\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=15,  # More epochs for better learning\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.2,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,\n",
    "    lr_scheduler_type=\"cosine\"  # Better learning rate schedule\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING NER MODEL\")\n",
    "print(\"=\"*60)\n",
    "trainer.train()\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a440c36b",
   "metadata": {},
   "source": [
    "## 7. Claim Extraction Function with Fixed Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa2d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_claims_with_ner(text, model, tokenizer, id2label, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Extract claims using NER model\n",
    "    IMPROVEMENTS:\n",
    "    - Better merging to prevent splitting\n",
    "    - Confidence threshold filtering\n",
    "    - Post-processing to remove weak/invalid claims\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    offset_mapping = inputs.pop('offset_mapping')[0]\n",
    "    \n",
    "    # Move to correct device\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0]\n",
    "    probabilities = torch.softmax(outputs.logits, dim=2)[0]\n",
    "    \n",
    "    # Build claims with better merging\n",
    "    claims = []\n",
    "    current_claim = None\n",
    "    \n",
    "    for idx, (pred, prob, (start, end)) in enumerate(zip(predictions, probabilities, offset_mapping)):\n",
    "        if start == 0 and end == 0:\n",
    "            continue\n",
    "        \n",
    "        label = id2label[pred.item()]\n",
    "        confidence = prob[pred].item()\n",
    "        \n",
    "        if label.startswith('B-'):\n",
    "            # Save previous claim\n",
    "            if current_claim:\n",
    "                claims.append(current_claim)\n",
    "            \n",
    "            # Start new claim\n",
    "            current_claim = {\n",
    "                'type': label[2:],\n",
    "                'start': start.item(),\n",
    "                'end': end.item(),\n",
    "                'confidence': confidence,\n",
    "                'token_count': 1\n",
    "            }\n",
    "        \n",
    "        elif label.startswith('I-') and current_claim:\n",
    "            # Continue current claim if same type\n",
    "            if label[2:] == current_claim['type']:\n",
    "                # Extend end position (merge tokens)\n",
    "                current_claim['end'] = end.item()\n",
    "                current_claim['token_count'] += 1\n",
    "                # Average confidence\n",
    "                current_claim['confidence'] = (\n",
    "                    current_claim['confidence'] * (current_claim['token_count'] - 1) + confidence\n",
    "                ) / current_claim['token_count']\n",
    "        \n",
    "        elif label == 'O':\n",
    "            # End current claim\n",
    "            if current_claim:\n",
    "                claims.append(current_claim)\n",
    "                current_claim = None\n",
    "    \n",
    "    # Don't forget last claim\n",
    "    if current_claim:\n",
    "        claims.append(current_claim)\n",
    "    \n",
    "    # Extract text and apply quality filters\n",
    "    filtered_claims = []\n",
    "    for claim in claims:\n",
    "        claim['text'] = text[claim['start']:claim['end']].strip()\n",
    "        claim.pop('token_count')\n",
    "        \n",
    "        # FILTER 1: Confidence threshold (removes weak predictions like \"To\" at 0.415)\n",
    "        if claim['confidence'] < confidence_threshold:\n",
    "            continue\n",
    "        \n",
    "        # FILTER 2: Minimum length (avoid single/double letter claims)\n",
    "        if len(claim['text']) < 3:\n",
    "            continue\n",
    "        \n",
    "        # FILTER 3: Avoid pure stopwords/prepositions\n",
    "        stopwords = {'to', 'a', 'an', 'the', 'of', 'in', 'on', 'at', 'by', 'for', 'with', 'from'}\n",
    "        if claim['text'].lower() in stopwords:\n",
    "            continue\n",
    "        \n",
    "        # FILTER 4: Must contain meaningful content (at least one alphanumeric)\n",
    "        if not any(c.isalnum() for c in claim['text']):\n",
    "            continue\n",
    "        \n",
    "        filtered_claims.append(claim)\n",
    "    \n",
    "    return filtered_claims\n",
    "\n",
    "print(\"Claim extraction function loaded with confidence filtering and post-processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf660ea",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eea409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating NER model on test set...\")\n",
    "results = trainer.predict(test_dataset)\n",
    "predictions = np.argmax(results.predictions, axis=2)\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for prediction, label in zip(predictions, results.label_ids):\n",
    "    true_label = []\n",
    "    pred_label = []\n",
    "    \n",
    "    for p, l in zip(prediction, label):\n",
    "        if l != -100:\n",
    "            true_label.append(id2label[l])\n",
    "            pred_label.append(id2label[p])\n",
    "    \n",
    "    true_labels.append(true_label)\n",
    "    pred_labels.append(pred_label)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLAIM-LEVEL NER PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(true_labels, pred_labels))\n",
    "\n",
    "print(f\"\\nOverall Token Accuracy: {accuracy_score(true_labels, pred_labels):.3f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test extraction on samples\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SET EXAMPLES - Claim Extraction Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ham_examples = [ex for ex in test_examples if len(ex['claim_spans']) == 0]\n",
    "smish_examples = [ex for ex in test_examples if len(ex['claim_spans']) > 0]\n",
    "\n",
    "sampled = random.sample(ham_examples, min(3, len(ham_examples))) + \\\n",
    "          random.sample(smish_examples, min(10, len(smish_examples)))\n",
    "\n",
    "for idx, example in enumerate(sampled, 1):\n",
    "    text = example['text']\n",
    "    gt_claims = example['claim_spans']\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Message: {text}\")\n",
    "    \n",
    "    print(f\"\\nGround Truth:\")\n",
    "    if gt_claims:\n",
    "        print(f\"  Label: SMISH ({len(gt_claims)} claims)\")\n",
    "        for i, claim in enumerate(gt_claims, 1):\n",
    "            print(f\"  {i}. {claim['label']:20} : '{claim['text']}'\")\n",
    "            print(f\"     Position: [{claim['start']}:{claim['end']}]\")\n",
    "    else:\n",
    "        print(f\"  Label: HAM (no claims)\")\n",
    "    \n",
    "    pred_claims = extract_claims_with_ner(text, model, tokenizer, id2label)\n",
    "    \n",
    "    print(f\"\\nModel Prediction:\")\n",
    "    if pred_claims:\n",
    "        print(f\"  Extracted {len(pred_claims)} claims:\")\n",
    "        for i, claim in enumerate(pred_claims, 1):\n",
    "            print(f\"  {i}. {claim['type']:20} : '{claim['text']}'\")\n",
    "            print(f\"     Position: [{claim['start']}:{claim['end']}]\")\n",
    "            print(f\"     Confidence: {claim['confidence']:.3f}\")\n",
    "    else:\n",
    "        print(f\"  No claims extracted\")\n",
    "    \n",
    "    print(f\"\\nAnalysis:\")\n",
    "    if len(gt_claims) == len(pred_claims):\n",
    "        print(f\"  Claim count: MATCH ({len(gt_claims)} claims)\")\n",
    "    else:\n",
    "        print(f\"  Claim count: MISMATCH (GT: {len(gt_claims)}, Pred: {len(pred_claims)})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a4b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed test results - PROPER NER EVALUATION\n",
    "print(\"\\nSaving detailed test results...\")\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for example in test_examples:\n",
    "    text = example['text']\n",
    "    gt_claims = example['claim_spans']\n",
    "    \n",
    "    # Get model predictions\n",
    "    pred_claims = extract_claims_with_ner(text, model, tokenizer, id2label)\n",
    "    \n",
    "    # Calculate claim-level metrics\n",
    "    matched_claims = 0\n",
    "    for pred in pred_claims:\n",
    "        for gt in gt_claims:\n",
    "            # Check for overlap\n",
    "            overlap_start = max(pred['start'], gt['start'])\n",
    "            overlap_end = min(pred['end'], gt['end'])\n",
    "            if overlap_end > overlap_start:\n",
    "                # Has overlap - check type match\n",
    "                gt_type = normalize_claim_type(gt['label'])\n",
    "                if pred['type'] == gt_type:\n",
    "                    matched_claims += 1\n",
    "                    break\n",
    "    \n",
    "    # Build result entry\n",
    "    result_entry = {\n",
    "        'id': example.get('id'),\n",
    "        'text': text,\n",
    "        'ground_truth': {\n",
    "            'num_claims': len(gt_claims),\n",
    "            'claims': [\n",
    "                {\n",
    "                    'type': claim['label'],\n",
    "                    'text': claim['text'],\n",
    "                    'start': claim['start'],\n",
    "                    'end': claim['end']\n",
    "                }\n",
    "                for claim in gt_claims\n",
    "            ]\n",
    "        },\n",
    "        'prediction': {\n",
    "            'num_claims': len(pred_claims),\n",
    "            'claims': [\n",
    "                {\n",
    "                    'type': claim['type'],\n",
    "                    'text': claim['text'],\n",
    "                    'start': claim['start'],\n",
    "                    'end': claim['end'],\n",
    "                    'confidence': round(claim['confidence'], 3)\n",
    "                }\n",
    "                for claim in pred_claims\n",
    "            ]\n",
    "        },\n",
    "        'evaluation': {\n",
    "            'claim_count_match': len(gt_claims) == len(pred_claims),\n",
    "            'claim_count_diff': len(pred_claims) - len(gt_claims),\n",
    "            'matched_claims': matched_claims,\n",
    "            'precision': matched_claims / len(pred_claims) if len(pred_claims) > 0 else 0,\n",
    "            'recall': matched_claims / len(gt_claims) if len(gt_claims) > 0 else (1 if len(pred_claims) == 0 else 0)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    test_results.append(result_entry)\n",
    "\n",
    "# Save to JSON\n",
    "results_path = f\"{save_dir}/test_results_detailed.json\"\n",
    "with open(results_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Calculate NER-specific metrics\n",
    "total = len(test_results)\n",
    "total_gt_claims = sum(len(r['ground_truth']['claims']) for r in test_results)\n",
    "total_pred_claims = sum(len(r['prediction']['claims']) for r in test_results)\n",
    "total_matched = sum(r['evaluation']['matched_claims'] for r in test_results)\n",
    "\n",
    "# Aggregate precision and recall\n",
    "overall_precision = total_matched / total_pred_claims if total_pred_claims > 0 else 0\n",
    "overall_recall = total_matched / total_gt_claims if total_gt_claims > 0 else 0\n",
    "overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "\n",
    "count_correct = sum(1 for r in test_results if r['evaluation']['claim_count_match'])\n",
    "\n",
    "summary = {\n",
    "    'total_test_examples': total,\n",
    "    'total_ground_truth_claims': total_gt_claims,\n",
    "    'total_predicted_claims': total_pred_claims,\n",
    "    'total_matched_claims': total_matched,\n",
    "    'metrics': {\n",
    "        'claim_count_accuracy': round(count_correct / total, 3),\n",
    "        'precision': round(overall_precision, 3),\n",
    "        'recall': round(overall_recall, 3),\n",
    "        'f1_score': round(overall_f1, 3)\n",
    "    },\n",
    "    'claim_statistics': {\n",
    "        'avg_claims_per_message_gt': round(total_gt_claims / total, 2),\n",
    "        'avg_claims_per_message_pred': round(total_pred_claims / total, 2),\n",
    "        'messages_with_claims_gt': sum(1 for r in test_results if len(r['ground_truth']['claims']) > 0),\n",
    "        'messages_with_claims_pred': sum(1 for r in test_results if len(r['prediction']['claims']) > 0)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "summary_path = f\"{save_dir}/test_results_summary.json\"\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TEST RESULTS SAVED - NER EVALUATION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Detailed results: {results_path}\")\n",
    "print(f\"Summary: {summary_path}\")\n",
    "print(f\"\\nClaim-Level NER Metrics:\")\n",
    "print(f\"  Total GT claims: {total_gt_claims}\")\n",
    "print(f\"  Total Predicted claims: {total_pred_claims}\")\n",
    "print(f\"  Matched claims: {total_matched}\")\n",
    "print(f\"\\n  Precision: {summary['metrics']['precision']:.1%}\")\n",
    "print(f\"  Recall: {summary['metrics']['recall']:.1%}\")\n",
    "print(f\"  F1 Score: {summary['metrics']['f1_score']:.1%}\")\n",
    "print(f\"\\n  Exact count match: {summary['metrics']['claim_count_accuracy']:.1%}\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62adaeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flexible evaluation with partial credit\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FLEXIBLE EVALUATION WITH PARTIAL CREDIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "flexible_results = []\n",
    "\n",
    "for example in test_examples:\n",
    "    text = example['text']\n",
    "    gt_claims = example['claim_spans']\n",
    "    pred_claims = extract_claims_with_ner(text, model, tokenizer, id2label)\n",
    "    \n",
    "    # Scoring system\n",
    "    score_details = {\n",
    "        'id': example.get('id'),\n",
    "        'text': text[:80] + '...' if len(text) > 80 else text,\n",
    "        'gt_count': len(gt_claims),\n",
    "        'pred_count': len(pred_claims),\n",
    "        'scores': {\n",
    "            'exact_match': 0,        # Perfect claim match\n",
    "            'partial_overlap': 0,    # Overlapping position but different boundaries\n",
    "            'type_match': 0,         # Same type but different position\n",
    "            'valid_detection': 0,    # Model found valid claim not in GT\n",
    "        },\n",
    "        'penalties': {\n",
    "            'wrong_type': 0,         # Detected claim but wrong type\n",
    "            'false_positive': 0,     # Detected claim where there isn't one\n",
    "            'missed_claim': 0        # Failed to detect GT claim\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    matched_gt = set()\n",
    "    matched_pred = set()\n",
    "    \n",
    "    # Match predictions to ground truth\n",
    "    for pred_idx, pred in enumerate(pred_claims):\n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for gt_idx, gt in enumerate(gt_claims):\n",
    "            if gt_idx in matched_gt:\n",
    "                continue\n",
    "            \n",
    "            # Calculate overlap\n",
    "            overlap_start = max(pred['start'], gt['start'])\n",
    "            overlap_end = min(pred['end'], gt['end'])\n",
    "            overlap_len = max(0, overlap_end - overlap_start)\n",
    "            \n",
    "            gt_len = gt['end'] - gt['start']\n",
    "            pred_len = pred['end'] - pred['start']\n",
    "            \n",
    "            # IoU (Intersection over Union)\n",
    "            union_len = gt_len + pred_len - overlap_len\n",
    "            iou = overlap_len / union_len if union_len > 0 else 0\n",
    "            \n",
    "            if iou > best_score:\n",
    "                best_score = iou\n",
    "                best_match = (gt_idx, gt, iou)\n",
    "        \n",
    "        if best_match:\n",
    "            gt_idx, gt, iou = best_match\n",
    "            \n",
    "            # Check type match\n",
    "            gt_type = normalize_claim_type(gt['label'])\n",
    "            pred_type = pred['type']\n",
    "            \n",
    "            if iou >= 0.7 and gt_type == pred_type:\n",
    "                # Exact match (high overlap + correct type)\n",
    "                score_details['scores']['exact_match'] += 1\n",
    "                matched_gt.add(gt_idx)\n",
    "                matched_pred.add(pred_idx)\n",
    "            \n",
    "            elif iou >= 0.3:\n",
    "                # Partial overlap\n",
    "                if gt_type == pred_type:\n",
    "                    score_details['scores']['partial_overlap'] += 0.5\n",
    "                    matched_gt.add(gt_idx)\n",
    "                    matched_pred.add(pred_idx)\n",
    "                else:\n",
    "                    score_details['penalties']['wrong_type'] += 1\n",
    "                    matched_pred.add(pred_idx)\n",
    "            \n",
    "            elif gt_type == pred_type:\n",
    "                # Same type but different location\n",
    "                score_details['scores']['type_match'] += 0.3\n",
    "                matched_pred.add(pred_idx)\n",
    "        \n",
    "        else:\n",
    "            # No overlap with any GT claim\n",
    "            # Check if this could be a valid claim that's missing from GT\n",
    "            # (e.g., confidence > 0.8 suggests model is confident)\n",
    "            if pred['confidence'] > 0.8:\n",
    "                score_details['scores']['valid_detection'] += 0.2\n",
    "            else:\n",
    "                score_details['penalties']['false_positive'] += 1\n",
    "            matched_pred.add(pred_idx)\n",
    "    \n",
    "    # Unmatched GT claims are missed\n",
    "    score_details['penalties']['missed_claim'] = len(gt_claims) - len(matched_gt)\n",
    "    \n",
    "    # Calculate final score\n",
    "    total_positive = sum(score_details['scores'].values())\n",
    "    total_negative = sum(score_details['penalties'].values())\n",
    "    \n",
    "    # Normalize score (0-1 scale)\n",
    "    max_possible = max(len(gt_claims), len(pred_claims), 1)\n",
    "    normalized_score = max(0, (total_positive - total_negative * 0.5) / max_possible)\n",
    "    \n",
    "    score_details['total_score'] = round(normalized_score, 3)\n",
    "    score_details['performance_category'] = (\n",
    "        'excellent' if normalized_score >= 0.8 else\n",
    "        'good' if normalized_score >= 0.6 else\n",
    "        'fair' if normalized_score >= 0.4 else\n",
    "        'poor'\n",
    "    )\n",
    "    \n",
    "    flexible_results.append(score_details)\n",
    "\n",
    "# Save flexible evaluation\n",
    "flexible_path = f\"{save_dir}/test_results_flexible.json\"\n",
    "with open(flexible_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(flexible_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Calculate aggregate statistics\n",
    "avg_score = sum(r['total_score'] for r in flexible_results) / len(flexible_results)\n",
    "excellent = sum(1 for r in flexible_results if r['performance_category'] == 'excellent')\n",
    "good = sum(1 for r in flexible_results if r['performance_category'] == 'good')\n",
    "fair = sum(1 for r in flexible_results if r['performance_category'] == 'fair')\n",
    "poor = sum(1 for r in flexible_results if r['performance_category'] == 'poor')\n",
    "\n",
    "total_exact = sum(r['scores']['exact_match'] for r in flexible_results)\n",
    "total_partial = sum(r['scores']['partial_overlap'] for r in flexible_results)\n",
    "total_type = sum(r['scores']['type_match'] for r in flexible_results)\n",
    "total_valid = sum(r['scores']['valid_detection'] for r in flexible_results)\n",
    "\n",
    "print(f\"\\nFlexible Evaluation Results:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Average Score: {avg_score:.3f}\")\n",
    "print(f\"\\nPerformance Distribution:\")\n",
    "print(f\"  Excellent (≥0.8): {excellent} ({excellent/len(flexible_results)*100:.1f}%)\")\n",
    "print(f\"  Good (≥0.6):      {good} ({good/len(flexible_results)*100:.1f}%)\")\n",
    "print(f\"  Fair (≥0.4):      {fair} ({fair/len(flexible_results)*100:.1f}%)\")\n",
    "print(f\"  Poor (<0.4):      {poor} ({poor/len(flexible_results)*100:.1f}%)\")\n",
    "print(f\"\\nPositive Scores:\")\n",
    "print(f\"  Exact matches:        {total_exact:.1f} (full points)\")\n",
    "print(f\"  Partial overlaps:     {total_partial:.1f} (0.5 points each)\")\n",
    "print(f\"  Type matches:         {total_type:.1f} (0.3 points each)\")\n",
    "print(f\"  Valid detections:     {total_valid:.1f} (0.2 points each)\")\n",
    "print(f\"\\nSaved to: {flexible_path}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show examples of each category\n",
    "print(\"\\nExample Cases:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for category in ['excellent', 'good', 'fair', 'poor']:\n",
    "    examples = [r for r in flexible_results if r['performance_category'] == category]\n",
    "    if examples:\n",
    "        ex = examples[0]\n",
    "        print(f\"\\n{category.upper()} Performance (score: {ex['total_score']}):\")\n",
    "        print(f\"  Text: {ex['text']}\")\n",
    "        print(f\"  GT claims: {ex['gt_count']}, Predicted: {ex['pred_count']}\")\n",
    "        print(f\"  Exact: {ex['scores']['exact_match']}, Partial: {ex['scores']['partial_overlap']}\")\n",
    "        print(f\"  Missed: {ex['penalties']['missed_claim']}, FP: {ex['penalties']['false_positive']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ab086e",
   "metadata": {},
   "source": [
    "## 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c1241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "print(\"\\nSaving model to Google Drive...\")\n",
    "\n",
    "model_path = f\"{save_dir}/final_model\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "config = {\n",
    "    'approach': 'approach5_pure_ner_fixed',\n",
    "    'training_date': str(pd.Timestamp.now()),\n",
    "    'model_name': MODEL_NAME,\n",
    "    'num_train_examples': len(train_dataset),\n",
    "    'claim_types': CLAIM_TYPES,\n",
    "    'label_mappings': {'label2id': label2id, 'id2label': {int(k): v for k, v in id2label.items()}},\n",
    "    'improvements': [\n",
    "        'Fixed word splitting with character-level labels',\n",
    "        'Better token merging in extraction',\n",
    "        'Resolved overlapping spans',\n",
    "        'Grouped rare claims',\n",
    "        'Used balanced deduplicated dataset'\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(f\"{save_dir}/config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to: {save_dir}\")\n",
    "print(\"Ready to use!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
