{
 "cells": [
 {
 "cell_type": "markdown",
 "id": "af22a242",
 "metadata": {},
 "source": [
 "# SMS Claim Extraction - Training on Colab\n",
 "\n",
 "This notebook trains all 4 approaches for claim extraction research."
 ]
 },
 {
 "cell_type": "markdown",
 "id": "2ed20678",
 "metadata": {},
 "source": [
 "## Setup Instructions\n",
 "\n",
 "**Before running:**\n",
 "1. Replace `YOUR_PROJECT_ID` with your GCP project ID\n",
 "2. Replace `your-bucket-name` with your GCS bucket name\n",
 "3. Make sure your GCS bucket exists and you have write permissions\n",
 "\n",
 "**What this notebook does:**\n",
 "- Uses K-Fold Cross-Validation (5 folds) for robust training\n",
 "- Keeps test set BLIND until final evaluation\n",
 "- Saves all checkpoints to Google Cloud Storage\n",
 "- No more Drive storage issues!"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "4c2d2916",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Clone repository\n",
 "!git clone https://github.com/iamdiluxedbutcooler/sms-claim-check.git\n",
 "%cd sms-claim-check"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "f122e658",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Install dependencies (including GCS support)\n",
 "!pip install -q transformers datasets torch scikit-learn pandas numpy seaborn matplotlib openai python-dotenv evaluate accelerate sentencepiece seqeval google-cloud-storage"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "017201aa",
 "metadata": {},
 "source": [
 "## Setup Google Cloud Storage\n",
 "\n",
 "Authenticate and configure GCS bucket for saving checkpoints and results."
 ]
 },
 {
 "cell_type": "markdown",
 "id": "48d1bcdd",
 "metadata": {},
 "source": [
 "## Update Code (if needed)\n",
 "\n",
 "Run this cell ONLY if you need to pull latest code updates. It will backup experiments first."
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "28e7619e",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Backup experiments before updating code\n",
 "!cp -r experiments /content/drive/MyDrive/sms-claim-check/backup_experiments_$(date +%Y%m%d_%H%M%S) 2>/dev/null || echo \"No experiments to backup yet\"\n",
 "\n",
 "# Pull latest code\n",
 "!git pull origin main\n",
 "\n",
 "# IMPORTANT: Restart runtime after pulling to reload modules\n",
 "print(\"\\n[WARNING] After pulling, go to Runtime > Restart runtime to reload updated code!\")\n",
 "print(\"Then continue from where you left off.\")"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "c809a328",
 "metadata": {},
 "outputs": [],
 "source": [
 "# QUICK FIX: Reload modules without restarting runtime\n",
 "import sys\n",
 "import importlib\n",
 "\n",
 "# Remove cached modules\n",
 "modules_to_reload = [m for m in sys.modules.keys() if m.startswith('src.')]\n",
 "for module in modules_to_reload:\n",
 " del sys.modules[module]\n",
 "\n",
 "# Reload\n",
 "import src.models\n",
 "import src.data\n",
 "\n",
 "print(\"[OK] Modules reloaded! Continue training.\")"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "153377aa",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Authenticate with Google Cloud\n",
 "from google.colab import auth\n",
 "auth.authenticate_user()\n",
 "\n",
 "# Configure GCS\n",
 "import os\n",
 "os.environ['GCLOUD_PROJECT'] = 'YOUR_PROJECT_ID' # Replace with your GCP project ID\n",
 "\n",
 "# Test GCS connection\n",
 "from google.cloud import storage\n",
 "client = storage.Client()\n",
 "\n",
 "# Set your bucket name (IMPORTANT: Replace with your actual bucket name)\n",
 "GCS_BUCKET_NAME = 'pleng_deposit_sms' # Updated with your bucket name\n",
 "bucket = client.bucket(GCS_BUCKET_NAME)\n",
 "\n",
 "print(f\" Authenticated and connected to GCS bucket: {GCS_BUCKET_NAME}\")\n",
 "print(f\" Project: {os.environ['GCLOUD_PROJECT']}\")\n",
 "print(f\" User: plengnaps@gmail.com\")\n",
 "\n",
 "# Test permissions\n",
 "try:\n",
 " list(bucket.list_blobs(max_results=1))\n",
 " print(\" Permissions OK - bucket is accessible!\")\n",
 "except Exception as e:\n",
 " print(f\" Permission Error: {e}\")\n",
 " print(\"\\n FIX: Grant 'Storage Object Admin' role to plengnaps@gmail.com\")\n",
 " print(f\" Run: gsutil iam ch user:plengnaps@gmail.com:objectAdmin gs://{GCS_BUCKET_NAME}\")\n",
 "\n",
 "print(\"\\nBucket ready for checkpoints and results!\")"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "2d7cdf8d",
 "metadata": {},
 "source": [
 "## Mount GCS Bucket as Local Filesystem\n",
 "\n",
 "This mounts your GCS bucket so training writes **directly** to GCS, not local disk."
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "8705aa43",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Install gcsfuse\n",
 "!echo \"deb https://packages.cloud.google.com/apt gcsfuse-$(lsb_release -c -s) main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
 "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
 "!sudo apt-get update\n",
 "!sudo apt-get install -y gcsfuse\n",
 "\n",
 "# Create mount point\n",
 "!mkdir -p /content/gcs_bucket\n",
 "\n",
 "# Mount GCS bucket\n",
 "!gcsfuse --implicit-dirs {GCS_BUCKET_NAME} /content/gcs_bucket\n",
 "\n",
 "print(f\" GCS bucket mounted at: /content/gcs_bucket\")\n",
 "print(f\" All training outputs will save directly to: gs://{GCS_BUCKET_NAME}\")\n",
 "print(f\"\\n Training will now write directly to GCS - no local storage used!\")"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "c3c7bd3c",
 "metadata": {},
 "source": [
 "## Update Configs to Save to GCS Mount\n",
 "\n",
 "Redirect all experiment outputs to the mounted GCS bucket."
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "8860e666",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Update all config files to use GCS mount path\n",
 "import yaml\n",
 "from pathlib import Path\n",
 "\n",
 "configs = [\n",
 " 'configs/entity_ner.yaml',\n",
 " 'configs/claim_ner.yaml',\n",
 " 'configs/contrastive.yaml',\n",
 " 'configs/hybrid_llm.yaml',\n",
 " 'configs/hybrid_claim_llm.yaml'\n",
 "]\n",
 "\n",
 "for config_file in configs:\n",
 " with open(config_file, 'r') as f:\n",
 " config = yaml.safe_load(f)\n",
 " \n",
 " # Update output_dir to use GCS mount\n",
 " approach_name = config['output_config']['output_dir'].split('/')[-1]\n",
 " config['output_config']['output_dir'] = f'/content/gcs_bucket/experiments/{approach_name}'\n",
 " \n",
 " with open(config_file, 'w') as f:\n",
 " yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
 " \n",
 " print(f\" Updated {config_file} -> /content/gcs_bucket/experiments/{approach_name}\")\n",
 "\n",
 "print(f\"\\n All configs now save directly to: gs://{GCS_BUCKET_NAME}/experiments/\")\n",
 "print(\" Checkpoints during training will write directly to GCS!\")"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "832986bb",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Check GPU\n",
 "import torch\n",
 "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
 "if torch.cuda.is_available():\n",
 " print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "8724d6ad",
 "metadata": {},
 "source": [
 "## Optional: Download Checkpoint from GCS\n",
 "\n",
 "If you need to resume training or load a previous checkpoint:"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "8099bc9d",
 "metadata": {},
 "outputs": [],
 "source": [
 "def download_from_gcs(gcs_folder_path, local_folder):\n",
 " \"\"\"Download folder from GCS to local\"\"\"\n",
 " client = storage.Client()\n",
 " bucket = client.bucket(GCS_BUCKET_NAME)\n",
 " \n",
 " # List all blobs with prefix\n",
 " blobs = bucket.list_blobs(prefix=gcs_folder_path)\n",
 " \n",
 " file_count = 0\n",
 " for blob in blobs:\n",
 " # Create local path\n",
 " relative_path = blob.name[len(gcs_folder_path):].lstrip('/')\n",
 " local_path = Path(local_folder) / relative_path\n",
 " \n",
 " # Create parent directories\n",
 " local_path.parent.mkdir(parents=True, exist_ok=True)\n",
 " \n",
 " # Download file\n",
 " blob.download_to_filename(str(local_path))\n",
 " file_count += 1\n",
 " \n",
 " print(f'[DOWNLOADED] {file_count} files from gs://{GCS_BUCKET_NAME}/{gcs_folder_path} -> {local_folder}')\n",
 "\n",
 "# Example: Download latest checkpoint for approach 1\n",
 "# download_from_gcs('checkpoints/approach1_entity_ner_latest', 'experiments/approach1_entity_ner')\n",
 "\n",
 "print(\" Download function ready. Uncomment example above to use.\")"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "f4cea5d3",
 "metadata": {},
 "outputs": [],
 "source": [
 "# GCS helper functions (optional - for manual sync if needed)\n",
 "from pathlib import Path\n",
 "from google.cloud import storage\n",
 "from datetime import datetime\n",
 "\n",
 "def list_gcs_experiments():\n",
 " \"\"\"List all experiments in GCS bucket\"\"\"\n",
 " client = storage.Client()\n",
 " bucket = client.bucket(GCS_BUCKET_NAME)\n",
 " \n",
 " blobs = bucket.list_blobs(prefix='experiments/')\n",
 " folders = set()\n",
 " for blob in blobs:\n",
 " parts = blob.name.split('/')\n",
 " if len(parts) >= 2:\n",
 " folders.add(parts[1])\n",
 " \n",
 " print(f\"Experiments in gs://{GCS_BUCKET_NAME}/experiments/:\")\n",
 " for folder in sorted(folders):\n",
 " print(f\" - {folder}\")\n",
 "\n",
 "def download_experiment_results(approach_name, local_path='./results'):\n",
 " \"\"\"Download specific experiment results from GCS\"\"\"\n",
 " client = storage.Client()\n",
 " bucket = client.bucket(GCS_BUCKET_NAME)\n",
 " \n",
 " blobs = bucket.list_blobs(prefix=f'experiments/{approach_name}/')\n",
 " \n",
 " file_count = 0\n",
 " for blob in blobs:\n",
 " relative_path = blob.name[len('experiments/'):] \n",
 " local_file = Path(local_path) / relative_path\n",
 " local_file.parent.mkdir(parents=True, exist_ok=True)\n",
 " blob.download_to_filename(str(local_file))\n",
 " file_count += 1\n",
 " \n",
 " print(f\" Downloaded {file_count} files to {local_path}\")\n",
 "\n",
 "print(' GCS helper functions loaded!')\n",
 "print(' - list_gcs_experiments() - List all experiments')\n",
 "print(' - download_experiment_results(approach_name) - Download results')"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "f2d6f88b",
 "metadata": {},
 "source": [
 "## Approach 1: Entity-based NER"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "c264eef9",
 "metadata": {},
 "outputs": [],
 "source": [
 "!python train_kfold.py --config configs/entity_ner.yaml --n_folds 5\n",
 "\n",
 "# Already saved to GCS automatically! Check: gs://{GCS_BUCKET_NAME}/experiments/approach1_entity_ner/"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "83e2d227",
 "metadata": {},
 "source": [
 "## Approach 2: Claim-based NER"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "615312e1",
 "metadata": {},
 "outputs": [],
 "source": [
 "!python train_kfold.py --config configs/claim_ner.yaml --n_folds 5\n",
 "\n",
 "# Already saved to GCS automatically!"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "530d38cc",
 "metadata": {},
 "source": [
 "## Download Results from GCS and Run Final Evaluation\n",
 "\n",
 "Now that training is complete, let's download the results and run comprehensive evaluation."
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "17269f47",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Download ONLY the final trained models from GCS (not all checkpoints!)\n",
 "import os\n",
 "from pathlib import Path\n",
 "\n",
 "print(\" Downloading Approach 1 (Entity-NER) final models...\")\n",
 "# Download each fold's model directory separately\n",
 "for fold in range(1, 6):\n",
 " print(f\" Downloading fold_{fold}/model...\")\n",
 " # Create the fold directory first\n",
 " !mkdir -p /content/results/approach1_entity_ner/fold_{fold}\n",
 " !gsutil -m cp -r gs://{GCS_BUCKET_NAME}/experiments/approach1_entity_ner/fold_{fold}/model /content/results/approach1_entity_ner/fold_{fold}/\n",
 "\n",
 "# Download kfold_results.json\n",
 "!gsutil cp gs://{GCS_BUCKET_NAME}/experiments/approach1_entity_ner/kfold_results.json /content/results/approach1_entity_ner/\n",
 "\n",
 "print(\"\\n Downloading Approach 2 (Claim-NER) final models...\")\n",
 "# Download each fold's model directory separately\n",
 "for fold in range(1, 6):\n",
 " print(f\" Downloading fold_{fold}/model...\")\n",
 " # Create the fold directory first\n",
 " !mkdir -p /content/results/approach2_claim_ner/fold_{fold}\n",
 " !gsutil -m cp -r gs://{GCS_BUCKET_NAME}/experiments/approach2_claim_ner/fold_{fold}/model /content/results/approach2_claim_ner/fold_{fold}/\n",
 "\n",
 "# Download kfold_results.json\n",
 "!gsutil cp gs://{GCS_BUCKET_NAME}/experiments/approach2_claim_ner/kfold_results.json /content/results/approach2_claim_ner/\n",
 "\n",
 "print(\"\\n Downloaded ONLY final models (no checkpoints)\")\n",
 "print(f\"\\nTotal size:\")\n",
 "!du -sh /content/results/"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "08b8e867",
 "metadata": {},
 "source": [
 "### View K-Fold Results Summary\n",
 "\n",
 "Check the cross-validation results and test performance for each approach."
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "ffff9d5f",
 "metadata": {},
 "outputs": [],
 "source": [
 "import json\n",
 "import pandas as pd\n",
 "\n",
 "def load_kfold_results(approach_path):\n",
 " \"\"\"Load and display K-fold results from a trained approach\"\"\"\n",
 " results_file = Path(approach_path) / 'kfold_results.json'\n",
 " \n",
 " if not results_file.exists():\n",
 " print(f\" Results not found: {results_file}\")\n",
 " return None\n",
 " \n",
 " with open(results_file, 'r') as f:\n",
 " results = json.load(f)\n",
 " \n",
 " return results\n",
 "\n",
 "# Load results for both approaches\n",
 "print(\"=\"*70)\n",
 "print(\"APPROACH 1: Entity-NER Results\")\n",
 "print(\"=\"*70)\n",
 "entity_results = load_kfold_results('/content/results/approach1_entity_ner')\n",
 "if entity_results:\n",
 " print(f\"\\n Cross-Validation Results ({entity_results['n_folds']} folds):\")\n",
 " cv_avg = entity_results['cv_validation_metrics']['average']\n",
 " for metric, value in cv_avg.items():\n",
 " print(f\" {metric}: {value:.4f}\")\n",
 " \n",
 " print(f\"\\n Test Set Results (BLIND):\")\n",
 " test_avg = entity_results['test_metrics']['average']\n",
 " for metric, value in test_avg.items():\n",
 " print(f\" {metric}: {value:.4f}\")\n",
 "\n",
 "print(\"\\n\" + \"=\"*70)\n",
 "print(\"APPROACH 2: Claim-NER Results\")\n",
 "print(\"=\"*70)\n",
 "claim_results = load_kfold_results('/content/results/approach2_claim_ner')\n",
 "if claim_results:\n",
 " print(f\"\\n Cross-Validation Results ({claim_results['n_folds']} folds):\")\n",
 " cv_avg = claim_results['cv_validation_metrics']['average']\n",
 " for metric, value in cv_avg.items():\n",
 " print(f\" {metric}: {value:.4f}\")\n",
 " \n",
 " print(f\"\\n Test Set Results (BLIND):\")\n",
 " test_avg = claim_results['test_metrics']['average']\n",
 " for metric, value in test_avg.items():\n",
 " print(f\" {metric}: {value:.4f}\")"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "ede6d577",
 "metadata": {},
 "source": [
 "### Generate Comparison Visualizations\n",
 "\n",
 "Create plots comparing the two approaches across all metrics."
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "98dd9788",
 "metadata": {},
 "outputs": [],
 "source": [
 "import matplotlib.pyplot as plt\n",
 "import seaborn as sns\n",
 "import numpy as np\n",
 "\n",
 "# Set style\n",
 "sns.set_style(\"whitegrid\")\n",
 "plt.rcParams['figure.figsize'] = (14, 10)\n",
 "\n",
 "# Extract metrics for comparison\n",
 "approaches = ['Entity-NER', 'Claim-NER']\n",
 "results_list = [entity_results, claim_results]\n",
 "\n",
 "# Metrics to compare (with eval_ prefix!)\n",
 "metrics_to_plot = ['eval_f1_mean', 'eval_precision_mean', 'eval_recall_mean', 'eval_accuracy_mean']\n",
 "metric_labels = ['F1', 'Precision', 'Recall', 'Accuracy']\n",
 "\n",
 "# Create figure with subplots\n",
 "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
 "fig.suptitle('Model Comparison: Entity-NER vs Claim-NER', fontsize=16, fontweight='bold')\n",
 "\n",
 "for idx, (metric, label) in enumerate(zip(metrics_to_plot, metric_labels)):\n",
 " ax = axes[idx // 2, idx % 2]\n",
 " \n",
 " # Extract CV and Test scores\n",
 " cv_scores = []\n",
 " test_scores = []\n",
 " \n",
 " for result in results_list:\n",
 " if result:\n",
 " cv_scores.append(result['cv_validation_metrics']['average'].get(metric, 0))\n",
 " test_scores.append(result['test_metrics']['average'].get(metric, 0))\n",
 " else:\n",
 " cv_scores.append(0)\n",
 " test_scores.append(0)\n",
 " \n",
 " # Plot grouped bars\n",
 " x = np.arange(len(approaches))\n",
 " width = 0.35\n",
 " \n",
 " bars1 = ax.bar(x - width/2, cv_scores, width, label='Cross-Validation', alpha=0.8, color='steelblue')\n",
 " bars2 = ax.bar(x + width/2, test_scores, width, label='Test (Blind)', alpha=0.8, color='coral')\n",
 " \n",
 " # Customize\n",
 " ax.set_ylabel(label, fontsize=12)\n",
 " ax.set_title(f'{label} Comparison', fontsize=13, fontweight='bold')\n",
 " ax.set_xticks(x)\n",
 " ax.set_xticklabels(approaches)\n",
 " ax.legend()\n",
 " ax.set_ylim([0, 1.0])\n",
 " ax.grid(axis='y', alpha=0.3)\n",
 " \n",
 " # Add value labels on bars\n",
 " for bars in [bars1, bars2]:\n",
 " for bar in bars:\n",
 " height = bar.get_height()\n",
 " ax.text(bar.get_x() + bar.get_width()/2., height,\n",
 " f'{height:.3f}',\n",
 " ha='center', va='bottom', fontsize=10)\n",
 "\n",
 "plt.tight_layout()\n",
 "plt.savefig('/content/model_comparison.png', dpi=300, bbox_inches='tight')\n",
 "plt.show()\n",
 "\n",
 "print(\" Comparison plot saved to /content/model_comparison.png\")\n",
 "print(\"\\n WARNING: F1, Precision, and Recall are all 0.0!\")\n",
 "print(\" This suggests the models may not have trained properly.\")\n",
 "print(\" Common causes:\")\n",
 "print(\" - Label mismatch during training\")\n",
 "print(\" - All predictions are 'O' (Outside) class\")\n",
 "print(\" - Training data preprocessing issue\")\n",
 "print(\" Check the training logs for errors!\")\n"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "ad856b2d",
 "metadata": {},
 "source": [
 "### K-Fold Variance Analysis\n",
 "\n",
 "Visualize the variance across folds to assess model stability."
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "f9187c05",
 "metadata": {},
 "outputs": [],
 "source": [
 "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
 "fig.suptitle('Cross-Validation Stability Analysis', fontsize=16, fontweight='bold')\n",
 "\n",
 "for idx, (approach_name, result) in enumerate([('Entity-NER', entity_results), ('Claim-NER', claim_results)]):\n",
 " if not result:\n",
 " continue\n",
 " \n",
 " ax = axes[idx]\n",
 " \n",
 " # Extract per-fold metrics\n",
 " per_fold = result['cv_validation_metrics']['per_fold']\n",
 " \n",
 " # Extract F1 scores for each fold (with eval_ prefix!)\n",
 " fold_f1s = []\n",
 " for fold_metrics in per_fold:\n",
 " # Find F1 metric (could be 'eval_f1', 'f1', etc.)\n",
 " f1_key = [k for k in fold_metrics.keys() if 'f1' in k.lower()][0]\n",
 " fold_f1s.append(fold_metrics[f1_key])\n",
 " \n",
 " # Plot\n",
 " folds = [f'Fold {i+1}' for i in range(len(fold_f1s))]\n",
 " bars = ax.bar(folds, fold_f1s, alpha=0.7, color='steelblue')\n",
 " \n",
 " # Add mean line\n",
 " mean_f1 = np.mean(fold_f1s)\n",
 " ax.axhline(y=mean_f1, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_f1:.4f}')\n",
 " \n",
 " # Add std shading\n",
 " std_f1 = np.std(fold_f1s)\n",
 " ax.fill_between(range(len(folds)), mean_f1 - std_f1, mean_f1 + std_f1, \n",
 " alpha=0.2, color='red', label=f'Std: ±{std_f1:.4f}')\n",
 " \n",
 " # Customize\n",
 " ax.set_title(f'{approach_name} - F1 Score per Fold', fontsize=13, fontweight='bold')\n",
 " ax.set_ylabel('F1 Score', fontsize=12)\n",
 " ax.set_xlabel('Fold', fontsize=12)\n",
 " ax.set_ylim([0, 1.0])\n",
 " ax.legend()\n",
 " ax.grid(axis='y', alpha=0.3)\n",
 " \n",
 " # Add value labels\n",
 " for bar, value in zip(bars, fold_f1s):\n",
 " height = bar.get_height()\n",
 " ax.text(bar.get_x() + bar.get_width()/2., height,\n",
 " f'{value:.4f}',\n",
 " ha='center', va='bottom', fontsize=10)\n",
 "\n",
 "plt.tight_layout()\n",
 "plt.savefig('/content/kfold_variance.png', dpi=300, bbox_inches='tight')\n",
 "plt.show()\n",
 "\n",
 "print(\" K-Fold variance plot saved to /content/kfold_variance.png\")\n",
 "\n",
 "# Check if all F1 scores are zero\n",
 "if entity_results and all(fold.get('eval_f1', 0) == 0 for fold in entity_results['cv_validation_metrics']['per_fold']):\n",
 " print(\"\\n WARNING: All Entity-NER F1 scores are 0.0 - model not learning!\")\n",
 "if claim_results and all(fold.get('eval_f1', 0) == 0 for fold in claim_results['cv_validation_metrics']['per_fold']):\n",
 " print(\" WARNING: All Claim-NER F1 scores are 0.0 - model not learning!\")\n"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "d7695170",
 "metadata": {},
 "source": [
 "### Generate Results Summary Table\n",
 "\n",
 "Create a clean comparison table for your research paper."
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "ca76eb39",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Create comprehensive results table\n",
 "results_data = []\n",
 "\n",
 "for approach_name, result in [('Entity-NER', entity_results), ('Claim-NER', claim_results)]:\n",
 " if not result:\n",
 " continue\n",
 " \n",
 " cv_metrics = result['cv_validation_metrics']['average']\n",
 " test_metrics = result['test_metrics']['average']\n",
 " \n",
 " # Extract key metrics (with eval_ prefix!)\n",
 " row = {\n",
 " 'Approach': approach_name,\n",
 " 'CV F1': f\"{cv_metrics.get('eval_f1_mean', 0):.4f}±{cv_metrics.get('eval_f1_std', 0):.4f}\",\n",
 " 'CV Precision': f\"{cv_metrics.get('eval_precision_mean', 0):.4f}±{cv_metrics.get('eval_precision_std', 0):.4f}\",\n",
 " 'CV Recall': f\"{cv_metrics.get('eval_recall_mean', 0):.4f}±{cv_metrics.get('eval_recall_std', 0):.4f}\",\n",
 " 'CV Accuracy': f\"{cv_metrics.get('eval_accuracy_mean', 0):.4f}±{cv_metrics.get('eval_accuracy_std', 0):.4f}\",\n",
 " 'Test F1': f\"{test_metrics.get('eval_f1_mean', 0):.4f}±{test_metrics.get('eval_f1_std', 0):.4f}\",\n",
 " 'Test Precision': f\"{test_metrics.get('eval_precision_mean', 0):.4f}±{test_metrics.get('eval_precision_std', 0):.4f}\",\n",
 " 'Test Recall': f\"{test_metrics.get('eval_recall_mean', 0):.4f}±{test_metrics.get('eval_recall_std', 0):.4f}\",\n",
 " 'Test Accuracy': f\"{test_metrics.get('eval_accuracy_mean', 0):.4f}±{test_metrics.get('eval_accuracy_std', 0):.4f}\",\n",
 " }\n",
 " results_data.append(row)\n",
 "\n",
 "# Create DataFrame\n",
 "df_results = pd.DataFrame(results_data)\n",
 "\n",
 "print(\"=\"*120)\n",
 "print(\"FINAL RESULTS SUMMARY\")\n",
 "print(\"=\"*120)\n",
 "print(df_results.to_string(index=False))\n",
 "print(\"=\"*120)\n",
 "\n",
 "# Save to CSV\n",
 "df_results.to_csv('/content/results_summary.csv', index=False)\n",
 "print(\"\\n Results summary saved to /content/results_summary.csv\")\n",
 "\n",
 "# Also create a LaTeX table for papers\n",
 "latex_table = df_results.to_latex(index=False, escape=False)\n",
 "with open('/content/results_summary.tex', 'w') as f:\n",
 " f.write(latex_table)\n",
 "print(\" LaTeX table saved to /content/results_summary.tex\")\n",
 "\n",
 "# Warning if metrics are all zero\n",
 "if all(row['CV F1'].startswith('0.0000') for row in results_data):\n",
 " print(\"\\n\" + \"=\"*120)\n",
 " print(\" CRITICAL WARNING: All F1 scores are 0.0!\")\n",
 " print(\"=\"*120)\n",
 " print(\"\\nPossible issues:\")\n",
 " print(\" 1. Label mismatch - check if label encoding matches during train/eval\")\n",
 " print(\" 2. All predictions are 'O' class - model only predicting 'Outside'\")\n",
 " print(\" 3. Tokenization issue - labels not aligned with tokens\")\n",
 " print(\" 4. Check training logs in GCS for actual errors\")\n",
 " print(\" 5. Verify preprocessor outputs match model expectations\")\n",
 " print(\"\\nNext steps:\")\n",
 " print(\" - Check GCS logs: gs://\" + GCS_BUCKET_NAME + \"/experiments/approach*/fold_*/\")\n",
 " print(\" - Run inference test (next cell) to see actual predictions\")\n",
 " print(\" - Verify training completed without errors\")\n",
 " print(\"=\"*120)\n"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "462babe6",
 "metadata": {},
 "source": [
 "### Run Inference on Sample Messages\n",
 "\n",
 "Test the trained models on some example SMS messages."
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "abda66a2",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Load best models and test on sample messages\n",
 "from src.models import EntityNERModel, ClaimNERModel\n",
 "\n",
 "# Sample phishing messages for testing\n",
 "test_messages = [\n",
 " \"URGENT: Your Amazon package #AB123 is delayed. Click bit.ly/pkg123 to reschedule within 24h or it will be returned.\",\n",
 " \"Your PayPal account has been locked due to suspicious activity. Call 1-800-FAKE-NUM immediately to verify your identity.\",\n",
 " \"WINNER! You've won $5000 in our lottery. Reply YES with your bank details to claim your prize now!\",\n",
 " \"Hi mom, this is my new number. Can you send me $200? My bank card is blocked and I need it urgently.\",\n",
 "]\n",
 "\n",
 "print(\"=\"*80)\n",
 "print(\"TESTING TRAINED MODELS ON SAMPLE MESSAGES\")\n",
 "print(\"=\"*80)\n",
 "\n",
 "# Test Entity-NER model\n",
 "print(\"\\n\" + \"=\"*80)\n",
 "print(\"APPROACH 1: Entity-NER Model\")\n",
 "print(\"=\"*80)\n",
 "\n",
 "entity_model = EntityNERModel({'model_name': 'roberta-base'})\n",
 "# Load best model from Fold 1 (you can choose any fold)\n",
 "entity_model.load('/content/results/approach1_entity_ner/fold_1/model')\n",
 "\n",
 "for i, msg in enumerate(test_messages, 1):\n",
 " print(f\"\\n Message {i}:\")\n",
 " print(f\" {msg}\")\n",
 " \n",
 " result = entity_model.predict(msg)\n",
 " print(f\"\\n Extracted Entities:\")\n",
 " for entity in result['entities']:\n",
 " print(f\" • {entity['label']}: '{entity['text']}'\")\n",
 "\n",
 "# Test Claim-NER model\n",
 "print(\"\\n\\n\" + \"=\"*80)\n",
 "print(\"APPROACH 2: Claim-NER Model\")\n",
 "print(\"=\"*80)\n",
 "\n",
 "claim_model = ClaimNERModel({'model_name': 'roberta-base'})\n",
 "claim_model.load('/content/results/approach2_claim_ner/fold_1/model')\n",
 "\n",
 "for i, msg in enumerate(test_messages, 1):\n",
 " print(f\"\\n Message {i}:\")\n",
 " print(f\" {msg}\")\n",
 " \n",
 " result = claim_model.predict(msg)\n",
 " print(f\"\\n Extracted Claims:\")\n",
 " if result['claims']:\n",
 " for claim in result['claims']:\n",
 " print(f\" • {claim}\")\n",
 " else:\n",
 " print(f\" (No claims detected)\")\n",
 "\n",
 "print(\"\\n\" + \"=\"*80)\n",
 "print(\" Inference testing complete!\")\n",
 "print(\"=\"*80)"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "97501e29",
 "metadata": {},
 "source": [
 "### Upload Plots and Results Back to GCS\n",
 "\n",
 "Save all generated visualizations and summaries back to your GCS bucket."
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "8827c99b",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Upload all generated visualizations and summaries to GCS\n",
 "!mkdir -p /content/final_results\n",
 "\n",
 "# Copy all generated files\n",
 "!cp /content/model_comparison.png /content/final_results/\n",
 "!cp /content/kfold_variance.png /content/final_results/\n",
 "!cp /content/results_summary.csv /content/final_results/\n",
 "!cp /content/results_summary.tex /content/final_results/\n",
 "\n",
 "# Upload to GCS\n",
 "print(\" Uploading results to GCS...\")\n",
 "!gsutil -m cp -r /content/final_results gs://{GCS_BUCKET_NAME}/analysis/\n",
 "\n",
 "print(\"\\n All results uploaded to GCS!\")\n",
 "print(f\"\\nView results at:\")\n",
 "print(f\" https://console.cloud.google.com/storage/browser/{GCS_BUCKET_NAME}/analysis/final_results/\")\n",
 "print(f\"\\nDownload with:\")\n",
 "print(f\" gsutil -m cp -r gs://{GCS_BUCKET_NAME}/analysis/final_results ./local_results/\")"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "7dd67f00",
 "metadata": {},
 "source": [
 "### Download Everything Locally (Optional)\n",
 "\n",
 "Create a zip file with all results for easy download."
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "e020c531",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Create a comprehensive zip file with everything\n",
 "!mkdir -p /content/complete_results\n",
 "!cp -r /content/results/* /content/complete_results/\n",
 "!cp -r /content/final_results/* /content/complete_results/\n",
 "\n",
 "# Zip it all\n",
 "!cd /content && zip -r complete_results.zip complete_results/\n",
 "\n",
 "print(\" Created complete_results.zip\")\n",
 "print(\"\\nContents:\")\n",
 "!zipinfo -1 /content/complete_results.zip | head -20\n",
 "\n",
 "# Download to your local machine\n",
 "from google.colab import files\n",
 "print(\"\\n Downloading zip file to your computer...\")\n",
 "files.download('/content/complete_results.zip')\n",
 "\n",
 "print(\"\\n\" + \"=\"*80)\n",
 "print(\" ALL DONE!\")\n",
 "print(\"=\"*80)\n",
 "print(\"\\nYou now have:\")\n",
 "print(\" K-Fold cross-validation results\")\n",
 "print(\" Test set evaluation (blind)\")\n",
 "print(\" Comparison plots\")\n",
 "print(\" Variance analysis\")\n",
 "print(\" Results summary table (CSV + LaTeX)\")\n",
 "print(\" Sample inference examples\")\n",
 "print(\" Everything backed up to GCS\")\n",
 "print(\"\\nNext steps:\")\n",
 "print(\" 1. Review the plots and tables\")\n",
 "print(\" 2. Write up your research findings\")\n",
 "print(\" 3. If needed, train Approach 4 (Contrastive) using same process\")\n",
 "print(\"=\"*80)"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "68aa0b2a",
 "metadata": {},
 "source": [
 "## Approach 4: Contrastive Learning"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "323d1ce0",
 "metadata": {},
 "outputs": [],
 "source": [
 "!python train_kfold.py --config configs/contrastive.yaml --n_folds 5\n",
 "\n",
 "# Already saved to GCS automatically!"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "054255e5",
 "metadata": {},
 "source": [
 "## Approach 3a: Hybrid Entity + LLM (Inference Only)"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "6291e4fb",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Set OpenAI API key\n",
 "import os\n",
 "os.environ['OPENAI_API_KEY'] = 'YOUR_API_KEY_HERE' # Replace with your key\n",
 "\n",
 "!python inference.py --config configs/hybrid_llm.yaml --model /content/gcs_bucket/experiments/approach1_entity_ner/best_model\n",
 "\n",
 "# Results saved to GCS automatically!"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "0c50ab90",
 "metadata": {},
 "source": [
 "## Approach 3b: Hybrid Claim + LLM (Inference Only)"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "0fa8b664",
 "metadata": {},
 "outputs": [],
 "source": [
 "!python inference.py --config configs/hybrid_claim_llm.yaml --model /content/gcs_bucket/experiments/approach2_claim_ner/best_model\n",
 "\n",
 "# Results saved to GCS automatically!"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "f4e2b2b3",
 "metadata": {},
 "source": [
 "## Compare Results"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "7a7a2657",
 "metadata": {},
 "outputs": [],
 "source": [
 "!python scripts/compare_models.py\n",
 "\n",
 "# Save final comparison to Drive\n",
 "save_all_results()"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "5bf115d3",
 "metadata": {},
 "source": [
 "## Download Results"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "d4f57939",
 "metadata": {},
 "outputs": [],
 "source": [
 "# List all experiments in GCS\n",
 "list_gcs_experiments()\n",
 "\n",
 "# Optional: Download results locally and zip\n",
 "print(\"\\nDownloading all results from GCS...\")\n",
 "download_experiment_results('approach1_entity_ner', './results')\n",
 "download_experiment_results('approach2_claim_ner', './results')\n",
 "download_experiment_results('approach4_contrastive', './results')\n",
 "\n",
 "!zip -r results.zip results/\n",
 "from google.colab import files\n",
 "files.download('results.zip')\n",
 "\n",
 "print(f'\\n[COMPLETE] All experiments saved to GCS!')\n",
 "print(f'View in console: https://console.cloud.google.com/storage/browser/{GCS_BUCKET_NAME}/experiments/')\n",
 "print(f'\\nTo view results directly: gsutil ls -r gs://{GCS_BUCKET_NAME}/experiments/')"
 ]
 }
 ],
 "metadata": {
 "language_info": {
 "name": "python"
 }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
