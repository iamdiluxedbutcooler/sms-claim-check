{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd311893",
   "metadata": {},
   "source": [
    "# Approach 1: Entity-First NER\n",
    "\n",
    "Two-step approach: Extract entities first, then parse them into claims.\n",
    "\n",
    "## Overview\n",
    "- **Step 1**: Train RoBERTa-based NER to extract entities (BRAND, PHONE, URL, EMAIL, etc.)\n",
    "- **Step 2**: Parse extracted entities into structured claims using rules\n",
    "- **Advantages**: Entities are concrete, clear intermediate representation, reusable\n",
    "- **Use Case**: When you need explicit entity extraction for other purposes\n",
    "\n",
    "## Entity Types\n",
    "- BRAND (Amazon, PayPal, IRS, etc.)\n",
    "- PHONE (phone numbers)\n",
    "- URL (links)\n",
    "- EMAIL (email addresses)\n",
    "- AMOUNT (monetary amounts)\n",
    "- DATE (time references)\n",
    "- ACCOUNT (account numbers/references)\n",
    "\n",
    "## Setup Instructions\n",
    "1. Upload `entity_annotations_2000.json` to Colab\n",
    "2. Run all cells in order\n",
    "3. Model extracts entities, then parses to claims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6eff47",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bb095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets accelerate seqeval scikit-learn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf4e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\" PyTorch version: {torch.__version__}\")\n",
    "print(f\" CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf8b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"Please upload '\n",
    "entity_annotations_2000.json\n",
    "'\")\n",
    "uploaded = files.upload()\n",
    "data_file = list(uploaded.keys())[0]\n",
    "print(f\"Uploaded: {data_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba20b90c",
   "metadata": {},
   "source": [
    "## 2. Define Entity Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb60bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define entity types\n",
    "ENTITY_TYPES = [\n",
    "    'BRAND',      # Company/organization names\n",
    "    'PHONE',      # Phone numbers\n",
    "    'URL',        # Web links\n",
    "    'EMAIL',      # Email addresses\n",
    "    'AMOUNT',     # Money/prizes\n",
    "    'DATE',       # Time references\n",
    "    'ACCOUNT',    # Account numbers/IDs\n",
    "    'PERSON',     # Person names\n",
    "    'LOCATION'    # Places\n",
    "]\n",
    "\n",
    "# Create BIO labels\n",
    "labels = ['O']  # Outside\n",
    "for entity_type in ENTITY_TYPES:\n",
    "    labels.append(f'B-{entity_type}')\n",
    "    labels.append(f'I-{entity_type}')\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"Total labels: {len(labels)}\")\n",
    "print(f\"Entity types: {len(ENTITY_TYPES)}\")\n",
    "print(f\"Labels: {labels[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04958456",
   "metadata": {},
   "source": [
    "## 3. Data Loading (Same as Approach 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db92d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bio_format(text, entity_spans):\n",
    "    \"\"\"\n",
    "    Convert text and entity spans to BIO format\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    bio_labels = ['O'] * len(words)\n",
    "    \n",
    "    char_pos = 0\n",
    "    \n",
    "    for word_idx, word in enumerate(words):\n",
    "        word_start = text.find(word, char_pos)\n",
    "        if word_start == -1:\n",
    "            continue\n",
    "            \n",
    "        word_end = word_start + len(word)\n",
    "        char_pos = word_end\n",
    "        \n",
    "        for span in entity_spans:\n",
    "            span_start = span['start']\n",
    "            span_end = span['end']\n",
    "            entity_label = span['label']\n",
    "            \n",
    "            if not (word_end <= span_start or word_start >= span_end):\n",
    "                if word_start <= span_start < word_end:\n",
    "                    bio_labels[word_idx] = f'B-{entity_label}'\n",
    "                else:\n",
    "                    if word_idx > 0 and bio_labels[word_idx-1] in [f'B-{entity_label}', f'I-{entity_label}']:\n",
    "                        bio_labels[word_idx] = f'I-{entity_label}'\n",
    "                    else:\n",
    "                        bio_labels[word_idx] = f'B-{entity_label}'\n",
    "                break\n",
    "    \n",
    "    return words, bio_labels\n",
    "\n",
    "def load_entity_data(json_file):\n",
    "    \"\"\"Load entity annotations\"\"\"\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    examples = []\n",
    "    \n",
    "    for entry in data:\n",
    "        text = entry['data']['text']\n",
    "        \n",
    "        if not entry.get('annotations') or len(entry['annotations']) == 0:\n",
    "            continue\n",
    "        \n",
    "        annotations = entry['annotations'][0]\n",
    "        \n",
    "        entity_spans = []\n",
    "        if 'result' in annotations and annotations['result']:\n",
    "            for result in annotations['result']:\n",
    "                value = result.get('value', {})\n",
    "                labels_list = value.get('labels', [])\n",
    "                \n",
    "                if labels_list:\n",
    "                    entity_spans.append({\n",
    "                        'text': value.get('text', ''),\n",
    "                        'start': value.get('start', 0),\n",
    "                        'end': value.get('end', 0),\n",
    "                        'label': labels_list[0]\n",
    "                    })\n",
    "        \n",
    "        tokens, bio_labels = convert_to_bio_format(text, entity_spans)\n",
    "        \n",
    "        examples.append({\n",
    "            'id': entry.get('id'),\n",
    "            'text': text,\n",
    "            'tokens': tokens,\n",
    "            'labels': bio_labels,\n",
    "            'entity_spans': entity_spans\n",
    "        })\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "examples = load_entity_data(data_file)\n",
    "print(f\" Loaded {len(examples)} examples\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\n First example with entities:\")\n",
    "for ex in examples[:5]:\n",
    "    if ex['entity_spans']:\n",
    "        print(f\"  Text: {ex['text'][:60]}...\")\n",
    "        print(f\"  Entities: {len(ex['entity_spans'])}\")\n",
    "        for span in ex['entity_spans'][:3]:\n",
    "            print(f\"    - {span['label']:10} : '{span['text']}'\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716baa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data with stratification (balanced ham/smish)\n",
    "# UPDATED: Match Approach 5 - 80/20 split, no validation set\n",
    "example_labels = []\n",
    "for ex in examples:\n",
    "    # SMISH = has entities, HAM = no entities\n",
    "    is_smish = len(ex.get('entity_spans', [])) > 0\n",
    "    example_labels.append('SMISH' if is_smish else 'HAM')\n",
    "\n",
    "# Stratified split to maintain ham/smish balance\n",
    "train_examples, test_examples = train_test_split(\n",
    "    examples, \n",
    "    test_size=0.20,  # Same as Approach 5\n",
    "    random_state=42,\n",
    "    stratify=example_labels\n",
    ")\n",
    "\n",
    "print(f\"Dataset split:\")\n",
    "print(f\"  Train: {len(train_examples)} examples\")\n",
    "print(f\"  Test:  {len(test_examples)} examples\")\n",
    "\n",
    "# Count ham/smish distribution\n",
    "from collections import Counter\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "for ex in train_examples:\n",
    "    is_smish = len(ex.get('entity_spans', [])) > 0\n",
    "    train_labels.append('SMISH' if is_smish else 'HAM')\n",
    "for ex in test_examples:\n",
    "    is_smish = len(ex.get('entity_spans', [])) > 0\n",
    "    test_labels.append('SMISH' if is_smish else 'HAM')\n",
    "\n",
    "train_dist = Counter(train_labels)\n",
    "test_dist = Counter(test_labels)\n",
    "print(f\"\\nTrain distribution:\")\n",
    "print(f\"  HAM:   {train_dist['HAM']} ({train_dist['HAM']/len(train_examples)*100:.1f}%)\")\n",
    "print(f\"  SMISH: {train_dist['SMISH']} ({train_dist['SMISH']/len(train_examples)*100:.1f}%)\")\n",
    "print(f\"\\nTest distribution:\")\n",
    "print(f\"  HAM:   {test_dist['HAM']} ({test_dist['HAM']/len(test_examples)*100:.1f}%)\")\n",
    "print(f\"  SMISH: {test_dist['SMISH']} ({test_dist['SMISH']/len(test_examples)*100:.1f}%)\")\n",
    "\n",
    "# Count labels\n",
    "all_labels = []\n",
    "for ex in train_examples:\n",
    "    all_labels.extend(ex['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953eb286",
   "metadata": {},
   "source": [
    "## 4. Tokenization and Model Training (Same as Approach 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b78a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n",
    "\n",
    "# Tokenization function (same as Approach 2)\n",
    "def tokenize_and_align_labels(examples, max_length=128):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        [ex['text'] for ex in examples],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_offsets_mapping=True,\n",
    "        is_split_into_words=False\n",
    "    )\n",
    "    \n",
    "    aligned_labels = []\n",
    "    \n",
    "    for i, example in enumerate(examples):\n",
    "        word_labels = example['labels']\n",
    "        text = example['text']\n",
    "        offset_mapping = tokenized_inputs['offset_mapping'][i]\n",
    "        \n",
    "        char_labels = ['O'] * len(text)\n",
    "        char_pos = 0\n",
    "        \n",
    "        for word, label in zip(example['tokens'], word_labels):\n",
    "            word_start = text.find(word, char_pos)\n",
    "            if word_start != -1:\n",
    "                word_end = word_start + len(word)\n",
    "                for j in range(word_start, word_end):\n",
    "                    char_labels[j] = label\n",
    "                char_pos = word_end\n",
    "        \n",
    "        labels = []\n",
    "        for start, end in offset_mapping:\n",
    "            if start == 0 and end == 0:\n",
    "                labels.append(-100)\n",
    "            else:\n",
    "                if start < len(char_labels):\n",
    "                    labels.append(label2id.get(char_labels[start], 0))\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "        \n",
    "        aligned_labels.append(labels)\n",
    "    \n",
    "    tokenized_inputs.pop('offset_mapping')\n",
    "    tokenized_inputs['labels'] = aligned_labels\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize - UPDATED: No validation set\n",
    "print(\"Tokenizing...\")\n",
    "train_tokenized = tokenize_and_align_labels(train_examples)\n",
    "test_tokenized = tokenize_and_align_labels(test_examples)\n",
    "print(\"Done\")\n",
    "\n",
    "# Create datasets\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "train_dataset = NERDataset(train_tokenized)\n",
    "test_dataset = NERDataset(test_tokenized)\n",
    "\n",
    "print(f\"Created datasets: Train={len(train_dataset)}, Test={len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931cec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\" Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb530ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        true_label = []\n",
    "        pred_label = []\n",
    "        \n",
    "        for p, l in zip(prediction, label):\n",
    "            if l != -100:\n",
    "                true_label.append(id2label[l])\n",
    "                pred_label.append(id2label[p])\n",
    "        \n",
    "        true_labels.append(true_label)\n",
    "        pred_labels.append(pred_label)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, pred_labels),\n",
    "        \"recall\": recall_score(true_labels, pred_labels),\n",
    "        \"f1\": f1_score(true_labels, pred_labels),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65789343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments - IMPROVED (Match Approach 5)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./entity-ner-model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    learning_rate=2e-5,  # Slightly lower for better convergence\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=15,  # More epochs for better learning\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.2,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,\n",
    "    lr_scheduler_type=\"cosine\"  # Better learning rate schedule\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bbcba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING ENTITY NER MODEL\")\n",
    "print(\"=\"*60)\n",
    "trainer.train()\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f074f",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce2670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "print(\" Evaluating...\")\n",
    "results = trainer.predict(test_dataset)\n",
    "predictions = np.argmax(results.predictions, axis=2)\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for prediction, label in zip(predictions, results.label_ids):\n",
    "    true_label = []\n",
    "    pred_label = []\n",
    "    \n",
    "    for p, l in zip(prediction, label):\n",
    "        if l != -100:\n",
    "            true_label.append(id2label[l])\n",
    "            pred_label.append(id2label[p])\n",
    "    \n",
    "    true_labels.append(true_label)\n",
    "    pred_labels.append(pred_label)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed test results\n",
    "print(\"\\nSaving detailed test results...\")\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for example in test_examples:\n",
    "    text = example['text']\n",
    "    gt_entities = example['entity_spans']\n",
    "    \n",
    "    # Get model predictions\n",
    "    pred_entities = extract_entities(text, model, tokenizer, id2label)\n",
    "    \n",
    "    # Calculate entity-level metrics - RELAXED MATCHING (>50% overlap)\n",
    "    matched_gt = set()\n",
    "    matched_pred = set()\n",
    "    \n",
    "    for i, pred in enumerate(pred_entities):\n",
    "        best_match = None\n",
    "        best_overlap = 0\n",
    "        \n",
    "        for j, gt in enumerate(gt_entities):\n",
    "            if j in matched_gt:\n",
    "                continue\n",
    "            \n",
    "            # Check overlap\n",
    "            overlap_start = max(pred['start'], gt['start'])\n",
    "            overlap_end = min(pred['end'], gt['end'])\n",
    "            overlap_len = max(0, overlap_end - overlap_start)\n",
    "            \n",
    "            # Calculate overlap percentage (relative to GT length)\n",
    "            gt_len = gt['end'] - gt['start']\n",
    "            overlap_ratio = overlap_len / gt_len if gt_len > 0 else 0\n",
    "            \n",
    "            # Require >50% overlap AND correct type\n",
    "            if overlap_ratio > 0.5 and pred['type'] == gt['label']:\n",
    "                if overlap_ratio > best_overlap:\n",
    "                    best_match = j\n",
    "                    best_overlap = overlap_ratio\n",
    "        \n",
    "        if best_match is not None:\n",
    "            matched_gt.add(best_match)\n",
    "            matched_pred.add(i)\n",
    "    \n",
    "    matched_entities = len(matched_gt)\n",
    "    \n",
    "    # Build result entry\n",
    "    result_entry = {\n",
    "        'id': example.get('id'),\n",
    "        'text': text,\n",
    "        'ground_truth': {\n",
    "            'num_entities': len(gt_entities),\n",
    "            'entities': [\n",
    "                {\n",
    "                    'type': entity['label'],\n",
    "                    'text': entity['text'],\n",
    "                    'start': entity['start'],\n",
    "                    'end': entity['end']\n",
    "                }\n",
    "                for entity in gt_entities\n",
    "            ]\n",
    "        },\n",
    "        'prediction': {\n",
    "            'num_entities': len(pred_entities),\n",
    "            'entities': [\n",
    "                {\n",
    "                    'type': entity['type'],\n",
    "                    'text': entity['text'],\n",
    "                    'start': entity['start'],\n",
    "                    'end': entity['end'],\n",
    "                    'confidence': round(entity['confidence'], 3)\n",
    "                }\n",
    "                for entity in pred_entities\n",
    "            ]\n",
    "        },\n",
    "        'evaluation': {\n",
    "            'entity_count_match': len(gt_entities) == len(pred_entities),\n",
    "            'entity_count_diff': len(pred_entities) - len(gt_entities),\n",
    "            'matched_entities': matched_entities,\n",
    "            'precision': len(matched_pred) / len(pred_entities) if len(pred_entities) > 0 else 0,\n",
    "            'recall': matched_entities / len(gt_entities) if len(gt_entities) > 0 else (1 if len(pred_entities) == 0 else 0)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    test_results.append(result_entry)\n",
    "\n",
    "# Save to Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "save_dir = '/content/drive/MyDrive/sms_claim_models/approach1_entity_ner'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "results_path = f\"{save_dir}/test_results_detailed.json\"\n",
    "with open(results_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Calculate aggregate metrics\n",
    "total = len(test_results)\n",
    "total_gt_entities = sum(len(r['ground_truth']['entities']) for r in test_results)\n",
    "total_pred_entities = sum(len(r['prediction']['entities']) for r in test_results)\n",
    "total_matched = sum(r['evaluation']['matched_entities'] for r in test_results)\n",
    "\n",
    "overall_precision = total_matched / total_pred_entities if total_pred_entities > 0 else 0\n",
    "overall_recall = total_matched / total_gt_entities if total_gt_entities > 0 else 0\n",
    "overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "\n",
    "count_correct = sum(1 for r in test_results if r['evaluation']['entity_count_match'])\n",
    "\n",
    "summary = {\n",
    "    'approach': 'approach1_entity_first_ner',\n",
    "    'evaluation_method': 'relaxed_matching_50pct_overlap',\n",
    "    'total_test_examples': total,\n",
    "    'total_ground_truth_entities': total_gt_entities,\n",
    "    'total_predicted_entities': total_pred_entities,\n",
    "    'total_matched_entities': total_matched,\n",
    "    'metrics': {\n",
    "        'entity_count_accuracy': round(count_correct / total, 3),\n",
    "        'precision': round(overall_precision, 3),\n",
    "        'recall': round(overall_recall, 3),\n",
    "        'f1_score': round(overall_f1, 3)\n",
    "    },\n",
    "    'entity_statistics': {\n",
    "        'avg_entities_per_message_gt': round(total_gt_entities / total, 2),\n",
    "        'avg_entities_per_message_pred': round(total_pred_entities / total, 2),\n",
    "        'messages_with_entities_gt': sum(1 for r in test_results if len(r['ground_truth']['entities']) > 0),\n",
    "        'messages_with_entities_pred': sum(1 for r in test_results if len(r['prediction']['entities']) > 0)\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = f\"{save_dir}/test_results_summary.json\"\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TEST RESULTS SAVED - RELAXED ENTITY MATCHING (>50% overlap)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Detailed results: {results_path}\")\n",
    "print(f\"Summary: {summary_path}\")\n",
    "print(f\"\\nEntity-Level Metrics (Relaxed):\")\n",
    "print(f\"  Total GT entities: {total_gt_entities}\")\n",
    "print(f\"  Total Predicted entities: {total_pred_entities}\")\n",
    "print(f\"  Matched entities: {total_matched}\")\n",
    "print(f\"\\n  Precision: {summary['metrics']['precision']:.1%}\")\n",
    "print(f\"  Recall: {summary['metrics']['recall']:.1%}\")\n",
    "print(f\"  F1 Score: {summary['metrics']['f1_score']:.1%}\")\n",
    "print(f\"\\n  Exact count match: {summary['metrics']['entity_count_accuracy']:.1%}\")\n",
    "print(f\"\\nNote: Accepts matches with >50% overlap with ground truth\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43df2ab",
   "metadata": {},
   "source": [
    "## 6. Entity Extraction + Claim Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a69552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text, model, tokenizer, id2label):\n",
    "    \"\"\"\n",
    "    Extract entities from text\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    offset_mapping = inputs.pop('offset_mapping')[0]\n",
    "    \n",
    "    # Move inputs to same device as model\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0]\n",
    "    probabilities = torch.softmax(outputs.logits, dim=2)[0]\n",
    "    \n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for idx, (pred, prob, (start, end)) in enumerate(zip(predictions, probabilities, offset_mapping)):\n",
    "        if start == 0 and end == 0:\n",
    "            continue\n",
    "        \n",
    "        label = id2label[pred.item()]\n",
    "        confidence = prob[pred].item()\n",
    "        \n",
    "        if label.startswith('B-'):\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            \n",
    "            current_entity = {\n",
    "                'type': label[2:],\n",
    "                'start': start.item(),\n",
    "                'end': end.item(),\n",
    "                'confidence': confidence\n",
    "            }\n",
    "        \n",
    "        elif label.startswith('I-') and current_entity:\n",
    "            if label[2:] == current_entity['type']:\n",
    "                current_entity['end'] = end.item()\n",
    "                current_entity['confidence'] = (current_entity['confidence'] + confidence) / 2\n",
    "        \n",
    "        elif label == 'O' and current_entity:\n",
    "            entities.append(current_entity)\n",
    "            current_entity = None\n",
    "    \n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    for entity in entities:\n",
    "        entity['text'] = text[entity['start']:entity['end']]\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def parse_entities_to_claims(entities, text):\n",
    "    \"\"\"\n",
    "    Parse extracted entities into structured claims\n",
    "    This is a rule-based approach\n",
    "    \"\"\"\n",
    "    claims = []\n",
    "    \n",
    "    # IDENTITY_CLAIM: BRAND entities\n",
    "    for entity in entities:\n",
    "        if entity['type'] == 'BRAND':\n",
    "            claims.append({\n",
    "                'type': 'IDENTITY_CLAIM',\n",
    "                'text': entity['text'],\n",
    "                'evidence': f\"Claims to be from {entity['text']}\",\n",
    "                'confidence': entity['confidence']\n",
    "            })\n",
    "    \n",
    "    # ACTION_CLAIM: PHONE or URL entities\n",
    "    for entity in entities:\n",
    "        if entity['type'] in ['PHONE', 'URL']:\n",
    "            claims.append({\n",
    "                'type': 'ACTION_CLAIM',\n",
    "                'text': entity['text'],\n",
    "                'evidence': f\"Requests action via {entity['type'].lower()}\",\n",
    "                'confidence': entity['confidence']\n",
    "            })\n",
    "    \n",
    "    # FINANCIAL_CLAIM: AMOUNT entities\n",
    "    for entity in entities:\n",
    "        if entity['type'] == 'AMOUNT':\n",
    "            claims.append({\n",
    "                'type': 'FINANCIAL_CLAIM',\n",
    "                'text': entity['text'],\n",
    "                'evidence': f\"Mentions money: {entity['text']}\",\n",
    "                'confidence': entity['confidence']\n",
    "            })\n",
    "    \n",
    "    # ACCOUNT_CLAIM: ACCOUNT entities\n",
    "    for entity in entities:\n",
    "        if entity['type'] == 'ACCOUNT':\n",
    "            claims.append({\n",
    "                'type': 'ACCOUNT_CLAIM',\n",
    "                'text': entity['text'],\n",
    "                'evidence': f\"References account: {entity['text']}\",\n",
    "                'confidence': entity['confidence']\n",
    "            })\n",
    "    \n",
    "    # URGENCY_CLAIM: Check for urgency keywords\n",
    "    urgency_keywords = ['urgent', 'now', 'immediately', 'asap', 'today', 'expires', '24 hours']\n",
    "    text_lower = text.lower()\n",
    "    for keyword in urgency_keywords:\n",
    "        if keyword in text_lower:\n",
    "            claims.append({\n",
    "                'type': 'URGENCY_CLAIM',\n",
    "                'text': keyword,\n",
    "                'evidence': f\"Uses urgency language: '{keyword}'\",\n",
    "                'confidence': 0.8\n",
    "            })\n",
    "            break\n",
    "    \n",
    "    return claims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab4adc",
   "metadata": {},
   "source": [
    "## 7. Complete Pipeline Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec15b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete pipeline\n",
    "test_messages = [\n",
    "    \"Your Amazon package is delayed. Click here urgently to reschedule delivery.\",\n",
    "    \"URGENT: Your PayPal account suspended. Call 0800-123-456 now to verify.\",\n",
    "    \"You've won £5000! Visit www.claim-prize.com immediately.\",\n",
    "]\n",
    "\n",
    "print(\" TWO-STEP PIPELINE: Entity Extraction → Claim Parsing\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, msg in enumerate(test_messages, 1):\n",
    "    print(f\"\\n{i}. Message: {msg}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Step 1: Extract entities\n",
    "    entities = extract_entities(msg, model, tokenizer, id2label)\n",
    "    print(f\"\\n   STEP 1 - Entities Extracted:\")\n",
    "    if entities:\n",
    "        for entity in entities:\n",
    "            print(f\"     - {entity['type']:10} : '{entity['text']}' (conf: {entity['confidence']:.2f})\")\n",
    "    else:\n",
    "        print(\"     (no entities found)\")\n",
    "    \n",
    "    # Step 2: Parse to claims\n",
    "    claims = parse_entities_to_claims(entities, msg)\n",
    "    print(f\"\\n   STEP 2 - Claims Parsed:\")\n",
    "    if claims:\n",
    "        for claim in claims:\n",
    "            print(f\"     - {claim['type']:20} : {claim['evidence']}\")\n",
    "    else:\n",
    "        print(\"     (no claims generated)\")\n",
    "    \n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138590d6",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (if not already mounted)\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "except:\n",
    "    print(\"Drive already mounted\")\n",
    "\n",
    "# Save to Google Drive\n",
    "save_dir = '/content/drive/MyDrive/sms_claim_models/approach1_entity_ner'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nSaving model to Google Drive: {save_dir}\")\n",
    "\n",
    "model_path = f\"{save_dir}/final_model\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "with open(f\"{save_dir}/label_mappings.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        'label2id': label2id,\n",
    "        'id2label': {int(k): v for k, v in id2label.items()},\n",
    "        'entity_types': ENTITY_TYPES\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"✓ Model saved to: {save_dir}\")\n",
    "print(f\"  - Model files: {model_path}/\")\n",
    "print(f\"  - Label mappings: {save_dir}/label_mappings.json\")\n",
    "print(\"\\nYou can access it from Google Drive!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5893ea79",
   "metadata": {},
   "source": [
    "## 9. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f45200",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"APPROACH 1: ENTITY-FIRST NER\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Two-step pipeline:\")\n",
    "print(f\"  1. Extract entities (BRAND, PHONE, URL, etc.)\")\n",
    "print(f\"  2. Parse entities → structured claims\")\n",
    "print(f\"\\nAdvantages:\")\n",
    "print(f\"   Entities are concrete and well-defined\")\n",
    "print(f\"   Clear intermediate representation\")\n",
    "print(f\"   Reusable entity extraction\")\n",
    "print(f\"\\nTest Metrics:\")\n",
    "print(f\"  Precision: {precision_score(true_labels, pred_labels):.3f}\")\n",
    "print(f\"  Recall:    {recall_score(true_labels, pred_labels):.3f}\")\n",
    "print(f\"  F1 Score:  {f1_score(true_labels, pred_labels):.3f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
