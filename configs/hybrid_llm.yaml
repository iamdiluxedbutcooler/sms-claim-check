# Configuration for Approach 3: Hybrid NER + LLM Pipeline
approach: "hybrid_llm"
name: "Hybrid NER + LLM"

model_config:
  # NER component config
  ner_config:
    model_name: "roberta-base"
    max_length: 128
    num_epochs: 10
    batch_size: 16
    learning_rate: 2e-5
    warmup_steps: 500
    weight_decay: 0.01
    seed: 42
  
  # LLM component config
  llm_provider: "openai"  # Options: "openai", "anthropic", "flan-t5"
  llm_model: "gpt-3.5-turbo"  # or "gpt-4", "claude-3-sonnet", "google/flan-t5-large"
  use_local_llm: false  # Set to true to use local T5 instead of API

data_config:
  annotations_file: "data/annotations/annotated_complete.json"
  train_ratio: 0.67
  val_ratio: 0.17
  test_ratio: 0.16
  seed: 42

output_config:
  experiment_name: "hybrid_ner_llm"
  output_dir: "experiments/hybrid_ner_llm"
  save_best_only: true
