{
 "cells": [
 {
 "cell_type": "markdown",
 "id": "a6dc468d",
 "metadata": {},
 "source": [
 "# Model Evaluation & Visualization Notebook\n",
 "\n",
 "## Quick Start Guide\n",
 "\n",
 "### For Google Colab Users:\n",
 "1. **Run the \"Setup & Imports\" cell** - It will auto-detect Colab\n",
 "2. **Run the \"Upload Result Files\" cell** - Upload your 4 JSON result files:\n",
 " - `approach1_entity_ner_results.json`\n",
 " - `approach2_claim_ner_results.json`\n",
 " - `approach3_hybrid_llm_results.json`\n",
 " - `approach4_contrastive_results.json`\n",
 "3. **Run remaining cells** to generate visualizations\n",
 "\n",
 "### For Local Users:\n",
 "1. Make sure result files are in the correct directories:\n",
 " - `experiments/approach1_entity_ner/results.json`\n",
 " - `experiments/approach2_claim_ner/results.json`\n",
 " - `experiments/approach3_hybrid_llm/results.json`\n",
 " - `experiments/approach4_contrastive/results.json`\n",
 "2. Run all cells\n",
 "\n",
 "---"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "cea18ad7",
 "metadata": {},
 "source": [
 "# SMS Phishing Detection: Model Evaluation & Visualization\n",
 "\n",
 "This notebook provides comprehensive evaluation and visualization of four different approaches:\n",
 "1. **Approach 1**: Entity-First NER Pipeline\n",
 "2. **Approach 2**: Claim-Phrase NER Pipeline\n",
 "3. **Approach 3**: Hybrid NER + LLM Pipeline\n",
 "4. **Approach 4**: Contrastive Learning Pipeline\n",
 "\n",
 "**Dataset**: Mendeley SMS Phishing Dataset\n",
 "- Training: 510 smishing messages\n",
 "- Test: 128 smishing messages\n",
 "- Note: This is ID (In-Distribution) evaluation only - no OOD test yet"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "431f2fcf",
 "metadata": {},
 "source": [
 "## Setup & Imports"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "74b7452a",
 "metadata": {},
 "outputs": [
 {
 "ename": "",
 "evalue": "",
 "output_type": "error",
 "traceback": [
 "\u001b[1;31mRunning cells with '.venv (Python 3.13.4)' requires the ipykernel package.\n",
 "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
 "\u001b[1;31mCommand: '/Users/pleng/Desktop/scammers/sms-claim-check/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
 ]
 }
 ],
 "source": [
 "import json\n",
 "import pandas as pd\n",
 "import numpy as np\n",
 "import matplotlib.pyplot as plt\n",
 "import seaborn as sns\n",
 "from pathlib import Path\n",
 "from typing import Dict, List, Tuple\n",
 "import warnings\n",
 "import os\n",
 "\n",
 "warnings.filterwarnings('ignore')\n",
 "\n",
 "# Set plotting style\n",
 "plt.style.use('seaborn-v0_8-darkgrid')\n",
 "sns.set_palette(\"husl\")\n",
 "\n",
 "# Detect environment (Colab vs local)\n",
 "try:\n",
 " import google.colab\n",
 " IN_COLAB = True\n",
 " print(\" Running in Google Colab\")\n",
 "except:\n",
 " IN_COLAB = False\n",
 " print(\" Running locally\")\n",
 "\n",
 "# Configuration\n",
 "if IN_COLAB:\n",
 " # For Colab, use current directory\n",
 " EXPERIMENTS_DIR = Path('./experiments')\n",
 " OUTPUT_DIR = Path('./experiments/evaluation_outputs')\n",
 "else:\n",
 " # For local, use relative path\n",
 " EXPERIMENTS_DIR = Path('../experiments')\n",
 " OUTPUT_DIR = Path('../experiments/evaluation_outputs')\n",
 "\n",
 "# Create directories with parents=True\n",
 "EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
 "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
 "\n",
 "print(\" Imports complete\")\n",
 "print(f\" Experiments directory: {EXPERIMENTS_DIR.absolute()}\")\n",
 "print(f\" Output directory: {OUTPUT_DIR.absolute()}\")"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "759ed915",
 "metadata": {},
 "source": [
 "## Upload Result Files (For Google Colab)"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "8c101c13",
 "metadata": {},
 "outputs": [],
 "source": [
 "if IN_COLAB:\n",
 " from google.colab import files\n",
 " import shutil\n",
 " \n",
 " print(\"=\" * 80)\n",
 " print(\"UPLOAD YOUR RESULT FILES\")\n",
 " print(\"=\" * 80)\n",
 " print(\"\\nPlease upload the 4 result JSON files:\")\n",
 " print(\" 1. approach1_entity_ner_results.json\")\n",
 " print(\" 2. approach2_claim_ner_results.json\")\n",
 " print(\" 3. approach3_hybrid_llm_results.json\")\n",
 " print(\" 4. approach4_contrastive_results.json\")\n",
 " print(\"\\nClick 'Choose Files' below and select all 4 files at once:\")\n",
 " print(\"-\" * 80)\n",
 " \n",
 " uploaded = files.upload()\n",
 " \n",
 " print(\"\\n\" + \"=\" * 80)\n",
 " print(f\" Uploaded {len(uploaded)} file(s)\")\n",
 " print(\"=\" * 80)\n",
 " \n",
 " # Create experiment directories and move files\n",
 " file_mapping = {\n",
 " 'approach1_entity_ner_results.json': 'approach1_entity_ner',\n",
 " 'approach2_claim_ner_results.json': 'approach2_claim_ner',\n",
 " 'approach3_hybrid_llm_results.json': 'approach3_hybrid_llm',\n",
 " 'approach4_contrastive_results.json': 'approach4_contrastive'\n",
 " }\n",
 " \n",
 " for filename, dir_name in file_mapping.items():\n",
 " if filename in uploaded:\n",
 " # Create approach directory\n",
 " approach_dir = EXPERIMENTS_DIR / dir_name\n",
 " approach_dir.mkdir(parents=True, exist_ok=True)\n",
 " \n",
 " # Move uploaded file to correct location\n",
 " dest_file = approach_dir / 'results.json'\n",
 " shutil.move(filename, str(dest_file))\n",
 " print(f\" Moved {filename} → {dest_file}\")\n",
 " else:\n",
 " print(f\" Warning: {filename} not uploaded\")\n",
 " \n",
 " print(\"\\n\" + \"=\" * 80)\n",
 " print(\" File organization complete!\")\n",
 " print(\"=\" * 80)\n",
 "else:\n",
 " print(\"⏭ Skipping upload (running locally - files already in place)\")"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "38e074c5",
 "metadata": {},
 "source": [
 "## Load Experimental Results"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "83ac6f47",
 "metadata": {},
 "outputs": [
 {
 "ename": "",
 "evalue": "",
 "output_type": "error",
 "traceback": [
 "\u001b[1;31mRunning cells with '.venv (Python 3.13.4)' requires the ipykernel package.\n",
 "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
 "\u001b[1;31mCommand: '/Users/pleng/Desktop/scammers/sms-claim-check/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
 ]
 }
 ],
 "source": [
 "def load_results(approach_dir: Path) -> Dict:\n",
 " \"\"\"Load results.json from an approach directory\"\"\"\n",
 " results_file = approach_dir / 'results.json'\n",
 " with open(results_file, 'r') as f:\n",
 " return json.load(f)\n",
 "\n",
 "# Load all results\n",
 "approaches = {\n",
 " 'approach1_entity_ner': 'Entity NER',\n",
 " 'approach2_claim_ner': 'Claim NER',\n",
 " 'approach3_hybrid_llm': 'Hybrid NER+LLM',\n",
 " 'approach4_contrastive': 'Contrastive Learning'\n",
 "}\n",
 "\n",
 "results = {}\n",
 "for dir_name, display_name in approaches.items():\n",
 " try:\n",
 " results[display_name] = load_results(EXPERIMENTS_DIR / dir_name)\n",
 " print(f\" Loaded: {display_name}\")\n",
 " except Exception as e:\n",
 " print(f\" Failed to load {display_name}: {e}\")\n",
 "\n",
 "print(f\"\\nTotal approaches loaded: {len(results)}\")"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "f0ef50d8",
 "metadata": {},
 "source": [
 "## Extract Key Metrics"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "601e383f",
 "metadata": {},
 "outputs": [],
 "source": [
 "def extract_metrics(results_dict: Dict) -> pd.DataFrame:\n",
 " \"\"\"Extract key metrics from all approaches into a DataFrame\"\"\"\n",
 " \n",
 " metrics_data = []\n",
 " \n",
 " for approach_name, result in results_dict.items():\n",
 " test_metrics = result['test_metrics']\n",
 " \n",
 " # Get phishing detection metrics (end-to-end performance)\n",
 " if 'end_to_end_phishing_detection' in test_metrics:\n",
 " detection = test_metrics['end_to_end_phishing_detection']\n",
 " \n",
 " metrics_data.append({\n",
 " 'Model': approach_name,\n",
 " 'Accuracy': detection['accuracy'],\n",
 " 'Precision': detection['precision'],\n",
 " 'Recall': detection['recall'],\n",
 " 'F1': detection['f1'],\n",
 " 'Support': detection['support'],\n",
 " 'TP': detection['confusion_matrix']['true_positive'],\n",
 " 'FP': detection['confusion_matrix']['false_positive'],\n",
 " 'TN': detection['confusion_matrix']['true_negative'],\n",
 " 'FN': detection['confusion_matrix']['false_negative']\n",
 " })\n",
 " elif 'in_distribution_performance' in test_metrics:\n",
 " # Contrastive approach uses different structure\n",
 " detection = test_metrics['in_distribution_performance']\n",
 " \n",
 " metrics_data.append({\n",
 " 'Model': approach_name,\n",
 " 'Accuracy': detection['accuracy'],\n",
 " 'Precision': detection['precision'],\n",
 " 'Recall': detection['recall'],\n",
 " 'F1': detection['f1'],\n",
 " 'Support': detection['support'],\n",
 " 'TP': detection['confusion_matrix']['true_positive'],\n",
 " 'FP': detection['confusion_matrix']['false_positive'],\n",
 " 'TN': detection['confusion_matrix']['true_negative'],\n",
 " 'FN': detection['confusion_matrix']['false_negative']\n",
 " })\n",
 " \n",
 " df = pd.DataFrame(metrics_data)\n",
 " return df\n",
 "\n",
 "df_metrics = extract_metrics(results)\n",
 "df_metrics = df_metrics.sort_values('F1', ascending=False).reset_index(drop=True)\n",
 "\n",
 "print(\"\\n\" + \"=\"*80)\n",
 "print(\"MODEL PERFORMANCE SUMMARY (ID Test Set - Mendeley Dataset)\")\n",
 "print(\"=\"*80)\n",
 "print(df_metrics.to_string(index=False))\n",
 "print(\"=\"*80)"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "e695de6a",
 "metadata": {},
 "source": [
 "## 1. Main Performance Table: ID Precision & Recall\n",
 "\n",
 "This table shows the core metrics for in-distribution (ID) performance.\n",
 "Since we don't have OOD test data yet, OOD columns are marked as N/A."
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "967211ab",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Prepare data for the main table\n",
 "fig, ax = plt.subplots(figsize=(14, 8))\n",
 "ax.axis('tight')\n",
 "ax.axis('off')\n",
 "\n",
 "# Table data with model IDs\n",
 "table_data = []\n",
 "table_data.append(['Model ID', 'Model Name', 'ID Precision (%)', 'ID Recall (%)', \n",
 " 'ID F1 (%)', 'OOD Precision (%)', 'OOD Recall (%)'])\n",
 "\n",
 "# Create model IDs\n",
 "model_ids = {\n",
 " 'Entity NER': 'app1_ent_ner',\n",
 " 'Claim NER': 'app2_clm_ner',\n",
 " 'Hybrid NER+LLM': 'app3_hyb_llm',\n",
 " 'Contrastive Learning': 'app4_con_lrn'\n",
 "}\n",
 "\n",
 "for _, row in df_metrics.iterrows():\n",
 " model_id = model_ids.get(row['Model'], 'unknown')\n",
 " table_data.append([\n",
 " model_id,\n",
 " row['Model'],\n",
 " f\"{row['Precision']*100:.2f}\",\n",
 " f\"{row['Recall']*100:.2f}\",\n",
 " f\"{row['F1']*100:.2f}\",\n",
 " 'N/A', # OOD not available yet\n",
 " 'N/A' # OOD not available yet\n",
 " ])\n",
 "\n",
 "# Color scheme\n",
 "colors = []\n",
 "colors.append(['#404040'] * 7) # Header row\n",
 "for i in range(1, len(table_data)):\n",
 " row_colors = ['#f0f0f0'] * 7\n",
 " colors.append(row_colors)\n",
 "\n",
 "table = ax.table(cellText=table_data, cellLoc='center', loc='center',\n",
 " cellColours=colors, bbox=[0, 0, 1, 1])\n",
 "\n",
 "table.auto_set_font_size(False)\n",
 "table.set_fontsize(10)\n",
 "table.scale(1, 2.5)\n",
 "\n",
 "# Style header row\n",
 "for i in range(7):\n",
 " table[(0, i)].set_facecolor('#404040')\n",
 " table[(0, i)].set_text_props(weight='bold', color='white')\n",
 "\n",
 "# Style data rows\n",
 "for i in range(1, len(table_data)):\n",
 " for j in range(7):\n",
 " table[(i, j)].set_edgecolor('white')\n",
 " table[(i, j)].set_linewidth(2)\n",
 " if j == 0: # Model ID column - bold\n",
 " table[(i, j)].set_text_props(weight='bold')\n",
 " if j >= 5: # OOD columns - italicize N/A\n",
 " table[(i, j)].set_text_props(style='italic', color='gray')\n",
 "\n",
 "plt.title('ID Performance: Precision, Recall & F1 Comparison\\n(OOD Results Pending)', \n",
 " fontsize=16, fontweight='bold', pad=20)\n",
 "\n",
 "plt.figtext(0.5, 0.02, \n",
 " 'Dataset: Mendeley SMS Phishing | Train: 510 smishing | Test: 128 smishing (ID only)',\n",
 " ha='center', fontsize=9, style='italic', color='#2c3e50')\n",
 "\n",
 "plt.tight_layout()\n",
 "plt.savefig(OUTPUT_DIR / 'performance_comparison_table.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
 "plt.show()\n",
 "\n",
 "print(\"\\n Table saved as 'performance_comparison_table.png'\")\n",
 "print(\"\\nNote: OOD evaluation pending - current results are ID (in-distribution) only\")"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "19aa9518",
 "metadata": {},
 "source": [
 "## 2. Performance Bar Charts"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "982580b0",
 "metadata": {},
 "outputs": [],
 "source": [
 "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
 "fig.suptitle('Model Performance Comparison (ID Test Set)', fontsize=18, fontweight='bold', y=0.995)\n",
 "\n",
 "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
 "colors_map = {\n",
 " 'Entity NER': '#3498db',\n",
 " 'Claim NER': '#e74c3c',\n",
 " 'Hybrid NER+LLM': '#2ecc71',\n",
 " 'Contrastive Learning': '#f39c12'\n",
 "}\n",
 "\n",
 "for idx, metric in enumerate(metrics_to_plot):\n",
 " ax = axes[idx // 2, idx % 2]\n",
 " \n",
 " bars = ax.bar(df_metrics['Model'], df_metrics[metric], \n",
 " color=[colors_map[m] for m in df_metrics['Model']],\n",
 " alpha=0.8, edgecolor='black', linewidth=1.5)\n",
 " \n",
 " # Add value labels on bars\n",
 " for bar in bars:\n",
 " height = bar.get_height()\n",
 " ax.text(bar.get_x() + bar.get_width()/2., height,\n",
 " f'{height:.3f}',\n",
 " ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
 " \n",
 " ax.set_ylabel(metric, fontsize=12, fontweight='bold')\n",
 " ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold', pad=10)\n",
 " ax.set_ylim([0, 1.1])\n",
 " ax.tick_params(axis='x', rotation=45)\n",
 " ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
 " \n",
 " # Add horizontal line at 0.9 for reference\n",
 " ax.axhline(y=0.9, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
 "\n",
 "plt.tight_layout()\n",
 "plt.savefig(OUTPUT_DIR / 'performance_bars.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
 "plt.show()\n",
 "\n",
 "print(\" Bar charts saved as 'performance_bars.png'\")"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "02a74d8e",
 "metadata": {},
 "source": [
 "## 3. Confusion Matrix Visualization"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "c0f704db",
 "metadata": {},
 "outputs": [],
 "source": [
 "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
 "fig.suptitle('Confusion Matrices - All Approaches', fontsize=18, fontweight='bold', y=0.995)\n",
 "\n",
 "for idx, (_, row) in enumerate(df_metrics.iterrows()):\n",
 " ax = axes[idx // 2, idx % 2]\n",
 " \n",
 " # Create confusion matrix\n",
 " cm = np.array([[row['TP'], row['FP']],\n",
 " [row['FN'], row['TN']]])\n",
 " \n",
 " # Plot heatmap\n",
 " sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
 " xticklabels=['Predicted Smishing', 'Predicted Ham'],\n",
 " yticklabels=['Actual Smishing', 'Actual Ham'],\n",
 " ax=ax, cbar_kws={'label': 'Count'},\n",
 " square=True, linewidths=2, linecolor='black')\n",
 " \n",
 " ax.set_title(f'{row[\"Model\"]}\\n(Acc: {row[\"Accuracy\"]:.3f}, F1: {row[\"F1\"]:.3f})',\n",
 " fontsize=13, fontweight='bold', pad=10)\n",
 " ax.set_ylabel('Actual', fontsize=11, fontweight='bold')\n",
 " ax.set_xlabel('Predicted', fontsize=11, fontweight='bold')\n",
 "\n",
 "plt.tight_layout()\n",
 "plt.savefig(OUTPUT_DIR / 'confusion_matrices.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
 "plt.show()\n",
 "\n",
 "print(\" Confusion matrices saved as 'confusion_matrices.png'\")"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "a2e9d6ca",
 "metadata": {},
 "source": [
 "## 4. Training Dynamics (Loss Curves)"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "cd77d339",
 "metadata": {},
 "outputs": [],
 "source": [
 "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
 "fig.suptitle('Training Dynamics - Loss Curves', fontsize=18, fontweight='bold', y=0.995)\n",
 "\n",
 "approach_names = list(results.keys())\n",
 "\n",
 "for idx, approach_name in enumerate(approach_names):\n",
 " ax = axes[idx // 2, idx % 2]\n",
 " \n",
 " train_metrics = results[approach_name]['train_metrics']\n",
 " \n",
 " # Extract loss curves\n",
 " if 'learning_curves' in train_metrics:\n",
 " train_losses = train_metrics['learning_curves']['train_losses']\n",
 " val_losses = train_metrics['learning_curves']['val_losses']\n",
 " epochs = range(1, len(train_losses) + 1)\n",
 " \n",
 " ax.plot(epochs, train_losses, marker='o', linewidth=2.5, \n",
 " label='Training Loss', color='#3498db', markersize=6)\n",
 " ax.plot(epochs, val_losses, marker='s', linewidth=2.5, \n",
 " label='Validation Loss', color='#e74c3c', markersize=6)\n",
 " \n",
 " # Mark best epoch\n",
 " best_epoch = train_metrics.get('best_epoch', len(epochs))\n",
 " ax.axvline(x=best_epoch, color='green', linestyle='--', \n",
 " alpha=0.7, linewidth=2, label=f'Best Epoch ({best_epoch})')\n",
 " \n",
 " ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
 " ax.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
 " ax.set_title(f'{approach_name}', fontsize=13, fontweight='bold', pad=10)\n",
 " ax.legend(loc='upper right', fontsize=10)\n",
 " ax.grid(True, alpha=0.3, linestyle='--')\n",
 " \n",
 " # Add early stopping info\n",
 " if train_metrics.get('early_stopped', False):\n",
 " ax.text(0.02, 0.98, 'Early Stopped', \n",
 " transform=ax.transAxes, fontsize=10,\n",
 " verticalalignment='top',\n",
 " bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
 "\n",
 "plt.tight_layout()\n",
 "plt.savefig(OUTPUT_DIR / 'training_loss_curves.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
 "plt.show()\n",
 "\n",
 "print(\" Loss curves saved as 'training_loss_curves.png'\")"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "a71ded1e",
 "metadata": {},
 "source": [
 "## 5. Entity/Claim-Level Performance (Where Applicable)"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "3a524831",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Entity NER performance breakdown\n",
 "entity_ner_results = results.get('Entity NER', {})\n",
 "if 'test_metrics' in entity_ner_results and 'entity_level' in entity_ner_results['test_metrics']:\n",
 " entity_metrics = entity_ner_results['test_metrics']['entity_level']['per_entity']\n",
 " \n",
 " # Create DataFrame\n",
 " entity_data = []\n",
 " for entity_type, metrics in entity_metrics.items():\n",
 " entity_data.append({\n",
 " 'Entity': entity_type,\n",
 " 'Precision': metrics['precision'],\n",
 " 'Recall': metrics['recall'],\n",
 " 'F1': metrics['f1'],\n",
 " 'Support': metrics['support']\n",
 " })\n",
 " \n",
 " df_entities = pd.DataFrame(entity_data).sort_values('F1', ascending=False)\n",
 " \n",
 " # Plot\n",
 " fig, ax = plt.subplots(figsize=(14, 8))\n",
 " \n",
 " x = np.arange(len(df_entities))\n",
 " width = 0.25\n",
 " \n",
 " ax.bar(x - width, df_entities['Precision'], width, label='Precision', \n",
 " color='#3498db', alpha=0.8, edgecolor='black')\n",
 " ax.bar(x, df_entities['Recall'], width, label='Recall', \n",
 " color='#e74c3c', alpha=0.8, edgecolor='black')\n",
 " ax.bar(x + width, df_entities['F1'], width, label='F1', \n",
 " color='#2ecc71', alpha=0.8, edgecolor='black')\n",
 " \n",
 " ax.set_xlabel('Entity Type', fontsize=12, fontweight='bold')\n",
 " ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
 " ax.set_title('Entity NER: Per-Entity Performance Breakdown', fontsize=16, fontweight='bold', pad=15)\n",
 " ax.set_xticks(x)\n",
 " ax.set_xticklabels(df_entities['Entity'], rotation=45, ha='right')\n",
 " ax.legend(fontsize=11)\n",
 " ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
 " ax.set_ylim([0, 1.1])\n",
 " \n",
 " # Add support counts\n",
 " for i, (idx, row) in enumerate(df_entities.iterrows()):\n",
 " ax.text(i, 1.05, f\"n={row['Support']}\", \n",
 " ha='center', fontsize=9, style='italic')\n",
 " \n",
 " plt.tight_layout()\n",
 " plt.savefig(OUTPUT_DIR / 'entity_ner_breakdown.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
 " plt.show()\n",
 " \n",
 " print(\" Entity breakdown saved as 'entity_ner_breakdown.png'\")\n",
 " print(\"\\nEntity Performance Summary:\")\n",
 " print(df_entities.to_string(index=False))"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "455d4813",
 "metadata": {},
 "source": [
 "## 6. Claim NER Performance Breakdown"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "7997fcbc",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Claim NER performance breakdown\n",
 "claim_ner_results = results.get('Claim NER', {})\n",
 "if 'test_metrics' in claim_ner_results and 'claim_level' in claim_ner_results['test_metrics']:\n",
 " claim_metrics = claim_ner_results['test_metrics']['claim_level']['per_claim_type']\n",
 " \n",
 " # Create DataFrame\n",
 " claim_data = []\n",
 " for claim_type, metrics in claim_metrics.items():\n",
 " claim_data.append({\n",
 " 'Claim Type': claim_type,\n",
 " 'Precision': metrics['precision'],\n",
 " 'Recall': metrics['recall'],\n",
 " 'F1': metrics['f1'],\n",
 " 'Support': metrics['support']\n",
 " })\n",
 " \n",
 " df_claims = pd.DataFrame(claim_data).sort_values('F1', ascending=False)\n",
 " \n",
 " # Plot\n",
 " fig, ax = plt.subplots(figsize=(14, 8))\n",
 " \n",
 " x = np.arange(len(df_claims))\n",
 " width = 0.25\n",
 " \n",
 " ax.bar(x - width, df_claims['Precision'], width, label='Precision', \n",
 " color='#9b59b6', alpha=0.8, edgecolor='black')\n",
 " ax.bar(x, df_claims['Recall'], width, label='Recall', \n",
 " color='#e67e22', alpha=0.8, edgecolor='black')\n",
 " ax.bar(x + width, df_claims['F1'], width, label='F1', \n",
 " color='#1abc9c', alpha=0.8, edgecolor='black')\n",
 " \n",
 " ax.set_xlabel('Claim Type', fontsize=12, fontweight='bold')\n",
 " ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
 " ax.set_title('Claim NER: Per-Claim-Type Performance Breakdown', fontsize=16, fontweight='bold', pad=15)\n",
 " ax.set_xticks(x)\n",
 " ax.set_xticklabels(df_claims['Claim Type'], rotation=45, ha='right')\n",
 " ax.legend(fontsize=11)\n",
 " ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
 " ax.set_ylim([0, 1.1])\n",
 " \n",
 " # Add support counts\n",
 " for i, (idx, row) in enumerate(df_claims.iterrows()):\n",
 " ax.text(i, 1.05, f\"n={row['Support']}\", \n",
 " ha='center', fontsize=9, style='italic')\n",
 " \n",
 " plt.tight_layout()\n",
 " plt.savefig(OUTPUT_DIR / 'claim_ner_breakdown.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
 " plt.show()\n",
 " \n",
 " print(\" Claim breakdown saved as 'claim_ner_breakdown.png'\")\n",
 " print(\"\\nClaim Performance Summary:\")\n",
 " print(df_claims.to_string(index=False))"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "cdc2d532",
 "metadata": {},
 "source": [
 "## 7. Inference Time Comparison"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "c412bb41",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Extract inference times\n",
 "inference_data = []\n",
 "\n",
 "for approach_name, result in results.items():\n",
 " test_metrics = result['test_metrics']\n",
 " \n",
 " # Different approaches store inference time differently\n",
 " if 'inference_time_ms' in test_metrics:\n",
 " inf_time = test_metrics['inference_time_ms']\n",
 " inference_data.append({\n",
 " 'Model': approach_name,\n",
 " 'Mean (ms)': inf_time.get('mean', 0),\n",
 " 'P50 (ms)': inf_time.get('p50', 0),\n",
 " 'P95 (ms)': inf_time.get('p95', 0),\n",
 " 'P99 (ms)': inf_time.get('p99', 0)\n",
 " })\n",
 " elif 'inference_pipeline' in test_metrics:\n",
 " inf_time = test_metrics['inference_pipeline']['total_time_ms']\n",
 " inference_data.append({\n",
 " 'Model': approach_name,\n",
 " 'Mean (ms)': inf_time.get('mean', 0),\n",
 " 'P50 (ms)': inf_time.get('p50', 0),\n",
 " 'P95 (ms)': inf_time.get('p95', 0),\n",
 " 'P99 (ms)': inf_time.get('p99', 0)\n",
 " })\n",
 " elif 'inference_efficiency' in test_metrics:\n",
 " inf_time = test_metrics['inference_efficiency']['encoding_time_ms']\n",
 " total_time = test_metrics['inference_efficiency'].get('total_inference_ms', inf_time['mean'])\n",
 " inference_data.append({\n",
 " 'Model': approach_name,\n",
 " 'Mean (ms)': total_time if isinstance(total_time, (int, float)) else inf_time['mean'],\n",
 " 'P50 (ms)': inf_time.get('p50', 0),\n",
 " 'P95 (ms)': inf_time.get('p95', 0),\n",
 " 'P99 (ms)': inf_time.get('p99', 0)\n",
 " })\n",
 "\n",
 "if inference_data:\n",
 " df_inference = pd.DataFrame(inference_data).sort_values('Mean (ms)')\n",
 " \n",
 " fig, ax = plt.subplots(figsize=(12, 7))\n",
 " \n",
 " x = np.arange(len(df_inference))\n",
 " width = 0.2\n",
 " \n",
 " ax.bar(x - 1.5*width, df_inference['Mean (ms)'], width, label='Mean', \n",
 " color='#3498db', alpha=0.8, edgecolor='black')\n",
 " ax.bar(x - 0.5*width, df_inference['P50 (ms)'], width, label='P50', \n",
 " color='#2ecc71', alpha=0.8, edgecolor='black')\n",
 " ax.bar(x + 0.5*width, df_inference['P95 (ms)'], width, label='P95', \n",
 " color='#f39c12', alpha=0.8, edgecolor='black')\n",
 " ax.bar(x + 1.5*width, df_inference['P99 (ms)'], width, label='P99', \n",
 " color='#e74c3c', alpha=0.8, edgecolor='black')\n",
 " \n",
 " ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
 " ax.set_ylabel('Inference Time (ms)', fontsize=12, fontweight='bold')\n",
 " ax.set_title('Inference Time Comparison (Lower is Better)', fontsize=16, fontweight='bold', pad=15)\n",
 " ax.set_xticks(x)\n",
 " ax.set_xticklabels(df_inference['Model'], rotation=45, ha='right')\n",
 " ax.legend(fontsize=10)\n",
 " ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
 " \n",
 " plt.tight_layout()\n",
 " plt.savefig(OUTPUT_DIR / 'inference_time_comparison.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
 " plt.show()\n",
 " \n",
 " print(\" Inference time comparison saved as 'inference_time_comparison.png'\")\n",
 " print(\"\\nInference Time Summary:\")\n",
 " print(df_inference.to_string(index=False))"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "77f93f8d",
 "metadata": {},
 "source": [
 "## 8. Radar Chart: Multi-Dimensional Comparison"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "06fae2f8",
 "metadata": {},
 "outputs": [],
 "source": [
 "from math import pi\n",
 "\n",
 "# Prepare data for radar chart\n",
 "categories = ['Precision', 'Recall', 'F1', 'Accuracy']\n",
 "N = len(categories)\n",
 "\n",
 "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
 "angles += angles[:1]\n",
 "\n",
 "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
 "\n",
 "for idx, (_, row) in enumerate(df_metrics.iterrows()):\n",
 " values = [row['Precision'], row['Recall'], row['F1'], row['Accuracy']]\n",
 " values += values[:1]\n",
 " \n",
 " ax.plot(angles, values, 'o-', linewidth=2.5, label=row['Model'], \n",
 " color=list(colors_map.values())[idx], markersize=8)\n",
 " ax.fill(angles, values, alpha=0.15, color=list(colors_map.values())[idx])\n",
 "\n",
 "ax.set_xticks(angles[:-1])\n",
 "ax.set_xticklabels(categories, fontsize=12, fontweight='bold')\n",
 "ax.set_ylim(0, 1)\n",
 "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
 "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=10)\n",
 "ax.grid(True, linestyle='--', alpha=0.7)\n",
 "\n",
 "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
 "plt.title('Multi-Metric Radar Comparison', fontsize=16, fontweight='bold', pad=30)\n",
 "\n",
 "plt.tight_layout()\n",
 "plt.savefig(OUTPUT_DIR / 'radar_chart.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
 "plt.show()\n",
 "\n",
 "print(\" Radar chart saved as 'radar_chart.png'\")"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "3763bd59",
 "metadata": {},
 "source": [
 "## 9. Summary Statistics & Key Insights"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "adf4d495",
 "metadata": {},
 "outputs": [],
 "source": [
 "print(\"\\n\" + \"=\"*80)\n",
 "print(\"KEY INSIGHTS & SUMMARY\")\n",
 "print(\"=\"*80)\n",
 "\n",
 "# Best performing model\n",
 "best_f1 = df_metrics.iloc[0]\n",
 "print(f\"\\n Best F1 Score: {best_f1['Model']} ({best_f1['F1']:.4f})\")\n",
 "\n",
 "best_precision = df_metrics.loc[df_metrics['Precision'].idxmax()]\n",
 "print(f\" Best Precision: {best_precision['Model']} ({best_precision['Precision']:.4f})\")\n",
 "\n",
 "best_recall = df_metrics.loc[df_metrics['Recall'].idxmax()]\n",
 "print(f\" Best Recall: {best_recall['Model']} ({best_recall['Recall']:.4f})\")\n",
 "\n",
 "best_accuracy = df_metrics.loc[df_metrics['Accuracy'].idxmax()]\n",
 "print(f\" Best Accuracy: {best_accuracy['Model']} ({best_accuracy['Accuracy']:.4f})\")\n",
 "\n",
 "# Error analysis\n",
 "print(\"\\n\" + \"-\"*80)\n",
 "print(\"ERROR ANALYSIS\")\n",
 "print(\"-\"*80)\n",
 "\n",
 "for _, row in df_metrics.iterrows():\n",
 " print(f\"\\n{row['Model']}:\")\n",
 " print(f\" False Positives: {row['FP']} (wrongly flagged as smishing)\")\n",
 " print(f\" False Negatives: {row['FN']} (missed smishing)\")\n",
 " fpr = row['FP'] / (row['FP'] + row['TN']) if (row['FP'] + row['TN']) > 0 else 0\n",
 " fnr = row['FN'] / (row['FN'] + row['TP']) if (row['FN'] + row['TP']) > 0 else 0\n",
 " print(f\" False Positive Rate: {fpr:.4f}\")\n",
 " print(f\" False Negative Rate: {fnr:.4f}\")\n",
 "\n",
 "# Dataset info\n",
 "print(\"\\n\" + \"-\"*80)\n",
 "print(\"DATASET INFORMATION\")\n",
 "print(\"-\"*80)\n",
 "print(\"Dataset: Mendeley SMS Phishing\")\n",
 "print(\"Training Set: 510 smishing messages\")\n",
 "print(\"Test Set: 128 smishing messages\")\n",
 "print(\"Evaluation Type: In-Distribution (ID) only\")\n",
 "print(\"OOD Evaluation: Pending (not yet available)\")\n",
 "\n",
 "print(\"\\n\" + \"=\"*80)"
 ]
 },
 {
 "cell_type": "markdown",
 "id": "45b61aae",
 "metadata": {},
 "source": [
 "## 10. Export Results Summary"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "id": "b0661651",
 "metadata": {},
 "outputs": [],
 "source": [
 "# Export comprehensive summary to CSV\n",
 "summary_file = OUTPUT_DIR / 'model_comparison_summary.csv'\n",
 "df_metrics.to_csv(summary_file, index=False)\n",
 "print(f\" Summary exported to: {summary_file}\")\n",
 "\n",
 "# Create a detailed report\n",
 "report = []\n",
 "report.append(\"SMS PHISHING DETECTION - MODEL EVALUATION REPORT\")\n",
 "report.append(\"=\"*80)\n",
 "report.append(f\"\\nDate: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
 "report.append(f\"Dataset: Mendeley SMS Phishing\")\n",
 "report.append(f\"Train Size: 510 smishing messages\")\n",
 "report.append(f\"Test Size: 128 smishing messages\")\n",
 "report.append(f\"Evaluation: In-Distribution (ID) only\\n\")\n",
 "\n",
 "report.append(\"\\nMODEL RANKINGS BY F1 SCORE\")\n",
 "report.append(\"-\"*80)\n",
 "for i, (_, row) in enumerate(df_metrics.iterrows(), 1):\n",
 " report.append(f\"{i}. {row['Model']}: F1={row['F1']:.4f} (P={row['Precision']:.4f}, R={row['Recall']:.4f})\")\n",
 "\n",
 "report.append(\"\\n\\nDETAILED METRICS\")\n",
 "report.append(\"=\"*80)\n",
 "report.append(df_metrics.to_string(index=False))\n",
 "\n",
 "report_text = \"\\n\".join(report)\n",
 "report_file = OUTPUT_DIR / 'evaluation_report.txt'\n",
 "with open(report_file, 'w') as f:\n",
 " f.write(report_text)\n",
 "\n",
 "print(f\" Detailed report exported to: {report_file}\")\n",
 "print(\"\\n\" + \"=\"*80)\n",
 "print(\"ALL VISUALIZATIONS AND REPORTS GENERATED SUCCESSFULLY!\")\n",
 "print(\"=\"*80)\n",
 "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
 "print(\"\\nGenerated files:\")\n",
 "for file in sorted(OUTPUT_DIR.glob('*')):\n",
 " print(f\" - {file.name}\")"
 ]
 }
 ],
 "metadata": {
 "kernelspec": {
 "display_name": ".venv",
 "language": "python",
 "name": "python3"
 },
 "language_info": {
 "name": "python",
 "version": "3.13.4"
 }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
